{"meta":{"title":"挖掘之城","subtitle":"weng-JJ技术小站","description":"Welcome to the world of a data miner","author":"wengJJ","url":"https://wengjj.ink","root":"/"},"pages":[{"title":"404 Not Found","date":"2020-08-30T14:00:17.442Z","updated":"2020-08-30T14:00:17.442Z","comments":true,"path":"index.html","permalink":"https://wengjj.ink/index.html","excerpt":"","text":"404 很抱歉，您访问的页面不存在 可能是输入地址有误或该地址已被删除"},{"title":"所有分类","date":"2020-08-30T13:08:10.368Z","updated":"2020-08-30T13:08:10.368Z","comments":true,"path":"categories/index.html","permalink":"https://wengjj.ink/categories/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2020-08-30T13:08:28.506Z","updated":"2020-08-30T13:08:28.506Z","comments":true,"path":"tags/index.html","permalink":"https://wengjj.ink/tags/index.html","excerpt":"","text":""},{"title":"我的朋友们","date":"2020-08-30T12:50:57.088Z","updated":"2020-08-30T12:50:57.088Z","comments":true,"path":"friends/index.html","permalink":"https://wengjj.ink/friends/index.html","excerpt":"这里写友链上方的内容。","text":"这里写友链上方的内容。 这里可以写友链页面下方的文字备注，例如自己的友链规范、示例等。"},{"title":"关于","date":"2020-08-31T11:08:48.391Z","updated":"2020-08-31T11:08:48.391Z","comments":true,"path":"about/index.html","permalink":"https://wengjj.ink/about/index.html","excerpt":"","text":"留言区 Stay Hungry Stay Foolish Your comments are very important to me"}],"posts":[{"title":"C++疑问集2-对象与指针","slug":"Cplus疑问集2","date":"2021-01-17T16:12:47.000Z","updated":"2021-01-17T16:25:42.824Z","comments":true,"path":"2021/01/18/Cplus疑问集2/","link":"","permalink":"https://wengjj.ink/2021/01/18/Cplus%E7%96%91%E9%97%AE%E9%9B%862/","excerpt":"2000年至今，由于以Loki、MPL(Boost)等程序库为代表的产生式编程和模板元编程的出现，C++出现了发展历史上又一个新的高峰，这些新技术的出现以及和原有技术的融合，使C++已经成为当今主流程序设计语言中最复杂的一员-维基百科C++词条 丰富的程序库以及本身的语言特性，使得C++在开发效率和运行效率上有着不错平衡，但这同时也增加了学习C++所带来的时间成本，C++疑问集此系列用于记录笔者学习C++路上遇到的问题，对部分易于混淆的知识点加以总结与汇总，以便于读者对C++有更深入的了解。","text":"2000年至今，由于以Loki、MPL(Boost)等程序库为代表的产生式编程和模板元编程的出现，C++出现了发展历史上又一个新的高峰，这些新技术的出现以及和原有技术的融合，使C++已经成为当今主流程序设计语言中最复杂的一员-维基百科C++词条 丰富的程序库以及本身的语言特性，使得C++在开发效率和运行效率上有着不错平衡，但这同时也增加了学习C++所带来的时间成本，C++疑问集此系列用于记录笔者学习C++路上遇到的问题，对部分易于混淆的知识点加以总结与汇总，以便于读者对C++有更深入的了解。 创建对象的两方法1A a1 = A; //在栈上创建一个A类的对象a1 创建对象时在栈上分配用于保存其数据成员的内存空间,其成员函数保存在公共代码区栈的内存分配由操作系统决定，所以函数执行完毕，操作系统会自动释放内存。 优点：系统自动分配和回收内存，数据存取块 缺点：栈的最大容量由系统预先设定好的，能获得的空间较小 12A a2 = new A(); //在堆上创建一个A类的对象a2delete a2; //释放对象a2的内存空间sd 创建对象时在堆上分配用于保存其数据成员的内存空间，其成员函数保存在公共代码区堆的内存分配由使用者决定，所以函数执行完毕，需要使用者手动delete对象 优点：堆的大小受限与有效的虚拟内存，能获得的空间大 缺点：由new分配的内存，系统响应速度较慢，容易产生内存碎片，忘记释放内存，容易导致内存泄漏 c++编译器内存分布全局数据区：分初始化区和未初始化区，存放全局变量(extern)，静态数据(static)常量区：存放常量(注意：const局部变量存储在栈区)代码区：函数代码的公共存放区域栈区：非new方法构造对象，存放对象的数据成员（注意：int main(){…}属于非new构造对象，其数据成员也存放在栈区）堆区：new方法构造对象，存放对象的数据成员Tips：c++每个对象所占用的存储空间只是该对象的数据成员（虚函数指针和虚基类指针也属于数据部分）所占用的存储空间，而函数代码则存放在公共函数代码的存储空间，两者的存储空间互不影响。 创建指针对象/创建引用的区别123int a = 1;int *p = &amp;a; //创建指针对象int &amp;r = a; //创建引用 创建指针对象 创建引用 内存空间 编译器为指针分配内存 编译器器不为引用分配内存，sizeof方法显示为所引用对象的内存 初始化 定义时可以不初始化 必须初始化 可否为空 指针可以为空 引用不可为空 指向性 指针可以指向其他对象 引用一旦创建不可以更改其指向 指针常量与常量指针 const int i = 1; const int *p1 = &i; int const *p2 = &i; int *const p3 = &i; 整形常量(const int) 指向常量的指针/*指针常量(pointer to const) 指向常量的指针/*指针常量(pointer to const) *常量指针(const pointer) 常量为只读状态，其值不可改变 指针指向的内容为常量，常量不可改变 指针指向的内容为常量，常量不可改变 指针自身为常量，始终指向同一个地址（定义时必须初始化） const int i = 1; i = 2;//Error,i为只读状态，其值不可改变（error: assignment of read-only variable ‘i’） int i =1;int j =2;const int *p1 = &i;i = 100;//OKp1 = &j;//OK*p1 = 100;//Error,*p1为常量，其值不可变（error: assignment of read-only location ‘* p1’）注意：*p1的值不可变指的不可通过*p1=100这种方式改变，可通过类似上方i = 100的方式改变其所指的值。 int i =1;int j =2;int const *p2 = &i;i = 100;//OKp2 = &j;//OK*p2 = 100;//Error,*p2为常量，其值不可变（error: assignment of read-only location ‘*p2’）注意：*p2的值不可变指的不可通过*p2=100这种方式改变，可通过类似上方i = 100的方式改变其所指的值。 int i =1;int j =2;int *const p3 = &i;i = 100;//OKp3 = &j;//Error,指针p3为常量，指针p3的地址不可变（error: assignment of read-only variable ‘p3’）*p3 = 100;//OK 注意：指针常量(pointer to const)和常量指针(const pointer)，这两个名词的中文翻译是经常引起争议的地方，而造成争议的地方就在于(const pointer)的中文翻译，详情见下表： 指针常量(pointer to const)和常量指针(const pointer) 常量指针(pointer to const)和指针常量(const pointer) 《c++ primer（第五版）中文版》2.4.2指针和const一节中，作者将(pointer to const)翻译为指向常量的指针，将(const pointer)翻译为常量指针，这是目前大家比较认同的翻译。而《c++ primer plus（第六版）中文版》为了避免歧义，作者将(pointer to const)翻译为指向const的指针，将(const pointer)翻译为const指针。 这种翻译方式来源于我们对常量的普遍翻译，例如(const int)一般我们将其称为整形常量，所以自然而然一部分程序员就会将(const pointer)翻译为指针常量。 才学疏浅，欢迎评论指导","categories":[{"name":"C++","slug":"C","permalink":"https://wengjj.ink/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://wengjj.ink/tags/C/"}]},{"title":"C++疑问集","slug":"Cplus疑问集","date":"2020-08-30T05:28:47.000Z","updated":"2021-01-17T15:44:04.272Z","comments":true,"path":"2020/08/30/Cplus疑问集/","link":"","permalink":"https://wengjj.ink/2020/08/30/Cplus%E7%96%91%E9%97%AE%E9%9B%86/","excerpt":"2000年至今，由于以Loki、MPL(Boost)等程序库为代表的产生式编程和模板元编程的出现，C++出现了发展历史上又一个新的高峰，这些新技术的出现以及和原有技术的融合，使C++已经成为当今主流程序设计语言中最复杂的一员-维基百科C++词条 丰富的程序库以及本身的语言特性，使得C++在开发效率和运行效率上有着不错平衡，但这同时也增加了学习C++所带来的时间成本，C++疑问集此系列用于记录笔者学习C++路上遇到的问题，对部分易于混淆的知识点加以总结与汇总，以便于读者对C++有更深入的了解。","text":"2000年至今，由于以Loki、MPL(Boost)等程序库为代表的产生式编程和模板元编程的出现，C++出现了发展历史上又一个新的高峰，这些新技术的出现以及和原有技术的融合，使C++已经成为当今主流程序设计语言中最复杂的一员-维基百科C++词条 丰富的程序库以及本身的语言特性，使得C++在开发效率和运行效率上有着不错平衡，但这同时也增加了学习C++所带来的时间成本，C++疑问集此系列用于记录笔者学习C++路上遇到的问题，对部分易于混淆的知识点加以总结与汇总，以便于读者对C++有更深入的了解。 十进制浮点数的二进制存储方式以及转换V = (-1)^s×M×2^E (-1)^s表示符号位，当s=0，V为正数；当s=1，V为负数 M表示有效数字，大于等于1，小于2 2^E表示指数 例如：将十进制178.125表示成机器内的32个字节的二进制形式 第一步:将178.125表示成二进制数:(178.125)(十进制数)=(10110010.001)(二进制形式); 十进制整数转换为二进制整数：==除2取余，逆序排列==十进制小数转换为二进制小数：==乘2取整，顺序排列== 第二步:将二进制形式的浮点实数转化为规格化的形式:(小数点向左移动7个二进制位可以得到) 10110010.001=1.0110010001*2^7 因而产生了以下三项: 符号位：该数为正数,故第31位为0,占一个二进制位 阶码：指数为7,故其阶码为127+7=134=(10000110)(二进制),占从第30到第23共8个二进制位 尾数：为小数点后的部分, 即0110010001.因为尾数共23个二进制位,在后面补13个0,即01100100010000000000000 所以178.125在内存中的实际表示方式为:0|10000110 |01100100010000000000000 cerr、cout、clog的区别 cout：标准输出流，用于常规输出，输入读进缓冲区，触发刷新缓冲区动作时输出缓冲区内容至终端显示器，也可被重定向输出至磁盘文件 cerr：非缓冲的标准错误流，用于显示错误信息，错误信息不输入缓冲区，直接显示在终端显示器，一般情况下不被重定向 clog：缓冲的标准错误流，一般用于记录运行日志，日志消息读进缓冲区，触发刷新缓冲区动作时输出缓冲区内容至终端显示器，一般情况下可被重定向输出至日志文件 tip：重定向在不同的操作环境定义不同 缓冲区的作用：计算机对缓冲区的操作快于磁盘操作，可以减少计算机对磁盘的读写次数，对低速的输入输出设备和高速的CPU之间进行协调工作。 触发刷新缓冲区的动作： 缓冲区满时 执行flush语句 行缓冲时遇到endl，cerr或cin时 关闭文件 cerr与clog的使用场景 clog：clog为缓冲输出，cerr为无缓冲输出，缓冲输出通常比无缓冲效率高，所以记录日志一般使用clog cerr：由于clog为缓冲输出，当程序崩溃时，如果所有日志信息以及错误信息都使用clog来输出，因为程序崩溃可能导致的缓冲区丢失，则可能看不到所有的信息。因此，我们一般使用cerr来输出错误信息，当程序崩溃时，由于cerr为无缓冲输出，则不会出现因为缓冲区丢失而导致的信息缺失的情况 C++ typedef的使用场景 定义类型的别名（区分#define的宏替换）例子：1234typedef char *pStr1;#define pStr2 char *;pStr1 s1, s2; //s1,s2为char*类型pStr2 s3, s4; //s3为char*类型，s4为char类型 注意：#define进行的宏替换类似于简单的replace操作，typedef相当于定义新的类型，编译器编译时会将typedef定义的类型认定为新的类型，详情见下方例子： 12345678typedef char* pStr;#define pStr1 char* ;const char* p1 = \"hello\";const pStr p2 = \"hello\";const pStr1 p3 = \"hello\";p1++;//正常，常量指针是可变的p2++;//报错，系统将pStr认定为常量的新类型，常量不可修改p3++;//正常，#define进行的是简单的文本替换，pStr1还是常量指针 定义struct结构体别名例子：123456typedef struct tagPOINT &#123; int x; int y; &#125;POINT; POINT p1; //省略了struct 定义多平台应用类型例子：定义浮点类型为REAL，只需要修改typedef就可以在多个平台使用REAL来定义浮点类型变量平台一支持long与double类型：typedef long double REAL;平台二支持long类型：typedef long REAL;平台三long与double类型都不支持：typedef float REAL; 注意：typedef跟static和register等存储类关键字一样，所以一行typedef声明中不能出现其他的存储类关键字（编译错误：typedef static int A;）4. 简化复杂的变量声明例子： 12345678910111213141516171819#include&lt;iostream&gt;using namespace std;typedef int A(char a,char b); //定义函数A，包含两个char类型形参，返回int类型值class X&#123; public:A(fun); //函数声明，等价于int fun(char a,char b);&#125;;int X::fun(char a,char b)&#123; cout&lt;&lt;\"succcess\"&lt;&lt;endl; return 0;&#125;int main()&#123; X *xx; xx-&gt;fun('a','b'); return 0;&#125; Java与C++的成员和类的默认访问修饰符？ Java：接口的默认修饰符为public;类的默认修饰符为default（整个包内都可以被访问） C++：类的默认修饰符为private（变量或函数在类的外部不可访问），而struct中则是public 才学疏浅，欢迎评论指导","categories":[{"name":"C++","slug":"C","permalink":"https://wengjj.ink/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://wengjj.ink/tags/C/"}]},{"title":"数据矿工学习-先有鸡or先有蛋？浅谈数据拆分与特征缩放的顺序问题","slug":"数据矿工学习-先有鸡or先有蛋？浅谈数据拆分与特征缩放的顺序问题","date":"2018-08-30T05:28:47.000Z","updated":"2020-08-30T08:00:46.454Z","comments":true,"path":"2018/08/30/数据矿工学习-先有鸡or先有蛋？浅谈数据拆分与特征缩放的顺序问题/","link":"","permalink":"https://wengjj.ink/2018/08/30/%E6%95%B0%E6%8D%AE%E7%9F%BF%E5%B7%A5%E5%AD%A6%E4%B9%A0-%E5%85%88%E6%9C%89%E9%B8%A1or%E5%85%88%E6%9C%89%E8%9B%8B%EF%BC%9F%E6%B5%85%E8%B0%88%E6%95%B0%E6%8D%AE%E6%8B%86%E5%88%86%E4%B8%8E%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE%E7%9A%84%E9%A1%BA%E5%BA%8F%E9%97%AE%E9%A2%98/","excerpt":"前些天在100-Days-Of-ML-Code上回答了一个关于数据拆分与特征缩放的顺序先后的一个issue，感觉挺有争议性的，故单独拎出来做下笔记说明。我的观点是：机器学习工程中，应该先进行数据划分，再进行特征缩放。出于严谨性，本篇文章是从机器学习-数据挖掘方面进行数据拆分与特征缩放的顺序问题阐述，同时也欢迎大家一起讨论这个问题。","text":"前些天在100-Days-Of-ML-Code上回答了一个关于数据拆分与特征缩放的顺序先后的一个issue，感觉挺有争议性的，故单独拎出来做下笔记说明。我的观点是：机器学习工程中，应该先进行数据划分，再进行特征缩放。出于严谨性，本篇文章是从机器学习-数据挖掘方面进行数据拆分与特征缩放的顺序问题阐述，同时也欢迎大家一起讨论这个问题。 问题阐述关于数据拆分与特征缩放的顺序先后问题，一般会在工程中遇到，具体表现为： 先数据拆分再特征缩放 123456from sklearn.preprocessing import StandardScaler,MinMaxScaler from sklearn.model_selection import train_test_splitX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.1) sc = StandardScaler() X_train = sc.fit_transform(X_train) X_test = sc.transform(X_test) 先数据缩放再数据拆分 1234from sklearn.preprocessing import StandardScaler,MinMaxScaler sc = StandardScaler() X_transform = sc.fit_transform(X) X_train,X_test,y_train,y_test = train_test_split(X_transform,y,test_size=0.1) 论点阐述首先先来看下我们常用的两种 sklearn 上的特征缩放：StandardScaler()与MinMaxScaler()从图中可以看出StandardScalar涉及到了均值μ与标准差σ，而MinMaxScaler则涉及到了最大值max与最小值min。这些参数的取值都得考虑到全局样本的，什么意思呢？我们来看下两者的输出结果： 先数据拆分再特征缩放 先数据缩放再数据拆分 可以很明显看出，两种不同的操作顺序输出的数据是完全不同的，也就是说样本的分布是完全不同的(很重要！后面阐述要用到)，那这种差异性在现实工程中会有什么影响？要解答这个问题，首先我们首先需要了解fit_transform()方法，fit_transform()你可以理解为fit()方法和transform()方法的pipeline，进行特征缩放时我们的顺序是 先fit获得相应的参数值（可以理解为获得特征缩放规则） 再用transform进行转换 fit_transform方法就是先执行fit()方法再执行transform()方法，所以每执行一次就会采用新的特征缩放规则，我们可以将训练集的特征缩放规则应用到测试集上，可以将测试集的特征缩放规则应用到训练集上(不过一般很少这么做)，但是通过全部数据集(训练集+测试集)fit到的的特征缩放规则是没有模型训练意义的。 这里我们举一个例子：假设农业部要求我们用LR模型来对花类型进行分类，我们经过学习得到了一个LR模型，模型上线后，现在需要对新的花数据进行预测分类（此时我们可以把旧花数据看做训练集，新花数据看做测试集）： 按照先数据拆分再特征缩放的做法是：先将旧花数据fit出特征缩放规则，接着将其transform到新花数据上，接着对应用旧花数据特征缩放规则的新花数据进行预测分类； 按照先数据缩放再数据拆分的做法是：将新旧花数据合并为一个总数据集，接着对总数据集进行fit_transform操作，最后再把新花数据切分出来进行预测分类； 重点！！！这时候问题来了，“我们经过学习得到了一个LR模型”，请问我们学习的数据是什么？旧花数据 OR 新旧花合并数据？答案肯定是旧花数据啊，更为详细地讲，是应用旧花数据特征缩放规则的旧花数据，这时候第二种做法的问题就出来了，我们这个LR模型是根据应用旧花数据特征缩放规则的旧花数据的分布学习到的这条分类线 而此时你却将这条分类线去应用在应用新旧花数据特征缩放规则的新花数据上，根据上方我们得到的论点“两种不同的操作顺序输出的样本的分布是完全不同”，两种完全不同的分布，你用根据其中一种分布学习得到分类线对另一种分布来说是完全没有使用意义的，因为两者根本可以说是根据不同的数据学习而来的，所以有些时候第二种做法效果可能会很好也可能会很糟糕，这就像你拿牛数据学习的LR模型去预测花的分类一样。而机器学习的前身就是统计学，而统计学的一个样本基本原则就是样本同质性（homogenetic）。 总结12345678910&gt;&gt;&gt; from sklearn.datasets import load_iris&gt;&gt;&gt; from sklearn.model_selection import train_test_split&gt;&gt;&gt; iris = load_iris()&gt;&gt;&gt; X, y = iris.data, iris.target&gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)&gt;&gt;&gt; quantile_transformer = preprocessing.QuantileTransformer(random_state=0)&gt;&gt;&gt; X_train_trans = quantile_transformer.fit_transform(X_train)&gt;&gt;&gt; X_test_trans = quantile_transformer.transform(X_test)&gt;&gt;&gt; np.percentile(X_train[:, 0], [0, 25, 50, 75, 100]) array([ 4.3, 5.1, 5.8, 6.5, 7.9]) 这里我贴的是sklearn的一段官方demo代码，可以看出sklearn的演示代码也是遵从先数据拆分再特征缩放的顺序进行的操作，先fit到X_train的特征缩放规则，再将其应用在X_test上，这也从一个小方面验证了我的观点吧(虽然我也不喜欢不严谨的举例论证方法)。所以综上所述，我的观点是在进行数据挖掘方面的工作时，在面对特征缩放环节时，应该先进行数据拆分再进行特征缩放环节。 才学疏浅，欢迎评论指导","categories":[{"name":"数据挖掘","slug":"数据挖掘","permalink":"https://wengjj.ink/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"}],"tags":[{"name":"数据拆分","slug":"数据拆分","permalink":"https://wengjj.ink/tags/%E6%95%B0%E6%8D%AE%E6%8B%86%E5%88%86/"},{"name":"特征缩放","slug":"特征缩放","permalink":"https://wengjj.ink/tags/%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE/"}]},{"title":"数据矿工学习-《统计学习方法》思维导图7.4-SMO序列最小最优化算法","slug":"数据矿工学习-《统计学习方法》思维导图7.4-SMO序列最小最优化算法","date":"2018-08-23T06:45:15.000Z","updated":"2020-08-30T08:00:53.664Z","comments":true,"path":"2018/08/23/数据矿工学习-《统计学习方法》思维导图7.4-SMO序列最小最优化算法/","link":"","permalink":"https://wengjj.ink/2018/08/23/%E6%95%B0%E6%8D%AE%E7%9F%BF%E5%B7%A5%E5%AD%A6%E4%B9%A0-%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE7.4-SMO%E5%BA%8F%E5%88%97%E6%9C%80%E5%B0%8F%E6%9C%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/","excerpt":"由Platt提出的SMO算法是支持向量机学习的一种快速算法，其特点为不断将原二次规划问题分解为只有两个变量的二次子规划问题，并对子问题进行解析求解，直到所有变量满足KKT条件为止。SVM是通过求得全局最优解来进行学习，SVM在面对大规模的训练样本时，效果往往不是很好·，SMO算法正是为了解决这个问题而提出的。至此SVM章节的相关内容就全部结束了（本节思维导图涉及较多的证明过程，各位可根据需要查阅）","text":"由Platt提出的SMO算法是支持向量机学习的一种快速算法，其特点为不断将原二次规划问题分解为只有两个变量的二次子规划问题，并对子问题进行解析求解，直到所有变量满足KKT条件为止。SVM是通过求得全局最优解来进行学习，SVM在面对大规模的训练样本时，效果往往不是很好·，SMO算法正是为了解决这个问题而提出的。至此SVM章节的相关内容就全部结束了（本节思维导图涉及较多的证明过程，各位可根据需要查阅） 思维来自《统计学习方法》-李航 凹脑图在线浏览地址：SMO序列最小最优化算法 才学疏浅，欢迎评论指导","categories":[{"name":"《统计学习方法-李航》","slug":"《统计学习方法-李航》","permalink":"https://wengjj.ink/categories/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9D%8E%E8%88%AA%E3%80%8B/"}],"tags":[{"name":"SMO序列最小最优化算法","slug":"SMO序列最小最优化算法","permalink":"https://wengjj.ink/tags/SMO%E5%BA%8F%E5%88%97%E6%9C%80%E5%B0%8F%E6%9C%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"}]},{"title":"数据矿工学习-Python数据可视化神器pyecharts使用细则","slug":"数据矿工学习-Python数据可视化神器pyecharts使用细则","date":"2018-08-16T06:22:34.000Z","updated":"2020-08-30T08:00:02.756Z","comments":true,"path":"2018/08/16/数据矿工学习-Python数据可视化神器pyecharts使用细则/","link":"","permalink":"https://wengjj.ink/2018/08/16/%E6%95%B0%E6%8D%AE%E7%9F%BF%E5%B7%A5%E5%AD%A6%E4%B9%A0-Python%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E7%A5%9E%E5%99%A8pyecharts%E4%BD%BF%E7%94%A8%E7%BB%86%E5%88%99/","excerpt":"注意：2018年的这篇文章是基于pyecharts-v0.5编写的，目前最新版本为v1.0，pyecharts-v1.0为船新的版本，详情请看gallery.pyecharts.org 前言我们都知道python上的一款可视化工具matplotlib,而前些阵子做一个Spark项目的时候用到了百度开源的一个可视化JS工具-Echarts，可视化类型非常多，但是得通过导入js库在Java Web项目上运行，平时用Python比较多，于是就在想有没有Python与Echarts结合的轮子。Google后，找到一个国人开发的一个Echarts与Python结合的轮子：pyecharts，下面就来简述下pyecharts一些使用细则。","text":"注意：2018年的这篇文章是基于pyecharts-v0.5编写的，目前最新版本为v1.0，pyecharts-v1.0为船新的版本，详情请看gallery.pyecharts.org 前言我们都知道python上的一款可视化工具matplotlib,而前些阵子做一个Spark项目的时候用到了百度开源的一个可视化JS工具-Echarts，可视化类型非常多，但是得通过导入js库在Java Web项目上运行，平时用Python比较多，于是就在想有没有Python与Echarts结合的轮子。Google后，找到一个国人开发的一个Echarts与Python结合的轮子：pyecharts，下面就来简述下pyecharts一些使用细则。 安装写这篇文章用的是Win环境，首先打开命令行(win+R),输入： 1pip install pyecharts 但笔者实测时发现，可能由于墙的原因，下载时会出现断线和速度过慢的问题导致下载失败，所以建议通过清华镜像来进行下载： 1pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pyecharts 出现上方的信息，即代表下载成功，我们可以来进行下一步的实验了！ 使用实例使用之前我们要强调一点：就是python2.x和python3.x的编码问题，在python3.x中你可以把它看做默认是unicode编码，但在python2.x中并不是默认的，原因就在它的bytes对象定义的混乱，而pyecharts是使用unicode编码来处理字符串和文件的，所以当你使用的是python2.x时，请务必在上方插入此代码： 1from __future__ import unicode_literals 现在我们来开始正式使用pyecharts，这里我们直接使用官方的数据： 柱状图-Bar1234567891011121314//导入柱状图-Bar from pyecharts import Bar//设置行名 columns = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"] //设置数据 data1 = [2.0, 4.9, 7.0, 23.2, 25.6, 76.7, 135.6, 162.2, 32.6, 20.0, 6.4, 3.3] data2 = [2.6, 5.9, 9.0, 26.4, 28.7, 70.7, 175.6, 182.2, 48.7, 18.8, 6.0, 2.3] //设置柱状图的主标题与副标题 bar = Bar(\"柱状图\", \"一年的降水量与蒸发量\") //添加柱状图的数据及配置项 bar.add(\"降水量\", columns, data1, mark_line=[\"average\"], mark_point=[\"max\", \"min\"]) bar.add(\"蒸发量\", columns, data2, mark_line=[\"average\"], mark_point=[\"max\", \"min\"]) //生成本地文件（默认为.html文件） bar.render() 运行结果如下： 简单的几行代码就可以将数据进行非常好看的可视化，而且还是动态的，在这里还是要安利一下jupyter，pyecharts在v0.1.9.2版本开始，在jupyter上直接调用实例（例如上方直接调用bar）就可以将图表直接表示出来，非常方便。 笔者数了数，目前pyecharts上的图表大概支持到二十多种，接下来，我们再用上方的数据来生成几个数据挖掘常用的图表示例： 饼图-Pie12345678//导入饼图Pie from pyecharts import Pie //设置主标题与副标题，标题设置居中，设置宽度为900 pie = Pie(\"饼状图\", \"一年的降水量与蒸发量\",title_pos='center',width=900) //加入数据，设置坐标位置为【25，50】，上方的colums选项取消显示 pie.add(\"降水量\", columns, data1 ,center=[25,50],is_legend_show=False) //加入数据，设置坐标位置为【75，50】，上方的colums选项取消显示，显示label标签 pie.add(\"蒸发量\", columns, data2 ,center=[75,50],is_legend_show=False,is_label_show=True) //保存图表 pie.render() 箱体图-Boxplot123456789//导入箱型图Boxplot from pyecharts import Boxplot boxplot = Boxplot(\"箱形图\", \"一年的降水量与蒸发量\") x_axis = ['降水量','蒸发量'] y_axis = [data1,data2] //prepare_data方法可以将数据转为嵌套的 [min, Q1, median (or Q2), Q3, max] yaxis = boxplot.prepare_data(y_axis) boxplot.add(\"天气统计\", x_axis, _yaxis) boxplot.render() 折线图-Line123456from pyecharts import Line line = Line(\"折线图\",\"一年的降水量与蒸发量\") //is_label_show是设置上方数据是否显示 line.add(\"降水量\", columns, data1, is_label_show=True) line.add(\"蒸发量\", columns, data2, is_label_show=True) line.render() 雷达图-Rader123456789101112from pyecharts import Radar radar = Radar(\"雷达图\", \"一年的降水量与蒸发量\") //由于雷达图传入的数据得为多维数据，所以这里需要做一下处理 radar_data1 = [[2.0, 4.9, 7.0, 23.2, 25.6, 76.7, 135.6, 162.2, 32.6, 20.0, 6.4, 3.3]] radar_data2 = [[2.6, 5.9, 9.0, 26.4, 28.7, 70.7, 175.6, 182.2, 48.7, 18.8, 6.0, 2.3]] //设置column的最大值，为了雷达图更为直观，这里的月份最大值设置有所不同 schema = [ (\"Jan\", 5), (\"Feb\",10), (\"Mar\", 10), (\"Apr\", 50), (\"May\", 50), (\"Jun\", 200), (\"Jul\", 200), (\"Aug\", 200), (\"Sep\", 50), (\"Oct\", 50), (\"Nov\", 10), (\"Dec\", 5) ] //传入坐标 radar.config(schema) radar.add(\"降水量\",radar_data1) //一般默认为同一种颜色，这里为了便于区分，需要设置item的颜色 radar.add(\"蒸发量\",radar_data2,item_color=\"#1C86EE\") radar.render() 散点图-scatter12345from pyecharts import Scatter scatter = Scatter(\"散点图\", \"一年的降水量与蒸发量\") //xais_name是设置横坐标名称，这里由于显示问题，还需要将y轴名称与y轴的距离进行设置 scatter.add(\"降水量与蒸发量的散点分布\", data1,data2,xaxis_name=\"降水量\",yaxis_name=\"蒸发量\", yaxis_name_gap=40) scatter.render() 图表布局 Grid由于标题与图表是属于两个不同的控件，所以这里必须对下方的图表Line进行标题位置设置，否则会出现标题重叠的bug。 123456789from pyecharts import Grid //设置折线图标题位置 line = Line(\"折线图\",\"一年的降水量与蒸发量\",title_top=\"45%\") line.add(\"降水量\", columns, data1, is_label_show=True) line.add(\"蒸发量\", columns, data2, is_label_show=True) grid = Grid() //设置两个图表的相对位置 grid.add(bar, grid_bottom=\"60%\") grid.add(line, grid_top=\"60%\") grid.render() 两图结合 Overlap123456from pyecharts import Overlap overlap = Overlap() bar = Bar(\"柱状图-折线图合并\", \"一年的降水量与蒸发量\") bar.add(\"降水量\", columns, data1, mark_point=[\"max\", \"min\"]) bar.add(\"蒸发量\", columns, data2, mark_point=[\"max\", \"min\"]) overlap.add(bar) overlap.add(line) overlap.render() 总结总结一下使用流程 导入相关图表包 进行图表的基础设置，创建图表对象 利用add()方法进行数据输入与图表设置(可以使用print_echarts_options()来输出所以可配置项) 利用render()方法来进行图表保存 pyecharts还有许多好玩的3D图表和地图图表，个人觉得地图图表是最好玩的，各位有兴趣可以去pyecharts的使用手册查看，有中文版的非常方便：pyecharts 参考资料： pyecharts使用手册：http://pyecharts.org/#/?id=pyecharts 才学疏浅，欢迎评论指导","categories":[{"name":"Python","slug":"Python","permalink":"https://wengjj.ink/categories/Python/"}],"tags":[{"name":"pyecharts","slug":"pyecharts","permalink":"https://wengjj.ink/tags/pyecharts/"}]},{"title":"数据矿工学习-这是一篇献给新手的深度学习综述","slug":"数据矿工学习-这是一篇献给新手的深度学习综述","date":"2018-08-10T07:56:28.000Z","updated":"2020-08-30T03:39:42.665Z","comments":true,"path":"2018/08/10/数据矿工学习-这是一篇献给新手的深度学习综述/","link":"","permalink":"https://wengjj.ink/2018/08/10/%E6%95%B0%E6%8D%AE%E7%9F%BF%E5%B7%A5%E5%AD%A6%E4%B9%A0-%E8%BF%99%E6%98%AF%E4%B8%80%E7%AF%87%E7%8C%AE%E7%BB%99%E6%96%B0%E6%89%8B%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0/","excerpt":"这篇综述论文列举出了近年来深度学习的重要研究成果，从方法、架构，以及正则化、优化技术方面进行概述。这篇综述对于刚入门的深度学习新手是一份不错的参考资料，在形成基本学术界图景、指导文献查找等方面都能提供帮助。 论文：Recent Advances in Deep Learning: An Overview 论文地址：https://arxiv.org/pdf/1807.08169v1.pdf","text":"这篇综述论文列举出了近年来深度学习的重要研究成果，从方法、架构，以及正则化、优化技术方面进行概述。这篇综述对于刚入门的深度学习新手是一份不错的参考资料，在形成基本学术界图景、指导文献查找等方面都能提供帮助。 论文：Recent Advances in Deep Learning: An Overview 论文地址：https://arxiv.org/pdf/1807.08169v1.pdf 摘要：深度学习是机器学习和人工智能研究的最新趋势之一。它也是当今最流行的科学研究趋势之一。深度学习方法为计算机视觉和机器学习带来了革命性的进步。新的深度学习技术正在不断诞生，超越最先进的机器学习甚至是现有的深度学习技术。近年来，全世界在这一领域取得了许多重大突破。由于深度学习正快度发展，导致了它的进展很难被跟进，特别是对于新的研究者。在本文中，我们将简要讨论近年来关于深度学习的最新进展。 1. 引言「深度学习」（DL）一词最初在 1986 被引入机器学习（ML），后来在 2000 年时被用于人工神经网络（ANN）。深度学习方法由多个层组成，以学习具有多个抽象层次的数据特征。DL 方法允许计算机通过相对简单的概念来学习复杂的概念。对于人工神经网络（ANN），深度学习（DL）（也称为分层学习（Hierarchical Learning））是指在多个计算阶段中精确地分配信用，以转换网络中的聚合激活。为了学习复杂的功能，深度架构被用于多个抽象层次，即非线性操作；例如 ANNs，具有许多隐藏层。用准确的话总结就是，深度学习是机器学习的一个子领域，它使用了多层次的非线性信息处理和抽象，用于有监督或无监督的特征学习、表示、分类和模式识别。 深度学习即表征学习是机器学习的一个分支或子领域，大多数人认为近代深度学习方法是从 2006 开始发展起来的。本文是关于最新的深度学习技术的综述，主要推荐给即将涉足该领域的研究者。本文包括 DL 的基本思想、主要方法、最新进展以及应用。 综述论文是非常有益的，特别是对某一特定领域的新研究人员。一个研究领域如果在不久的将来及相关应用领域中有很大的价值，那通常很难被实时跟踪到最新进展。现在，科学研究是一个很有吸引力的职业，因为知识和教育比以往任何时候都更容易分享和获得。对于一种技术研究的趋势来说，唯一正常的假设是它会在各个方面有很多的改进。几年前对某个领域的概述，现在可能已经过时了。 考虑到近年来深度学习的普及和推广，我们简要概述了深度学习和神经网络（NN），以及它的主要进展和几年来的重大突破。我们希望这篇文章将帮助许多新手研究者在这一领域全面了解最近的深度学习的研究和技术，并引导他们以正确的方式开始。同时，我们希望通过这项工作，向这个时代的顶级 DL 和 ANN 研究者们致敬：Geoffrey Hinton（Hinton）、Juergen Schmidhuber（Schmidhuber）、Yann LeCun（LeCun）、Yoshua Bengio（Bengio）和许多其他研究学者，他们的研究构建了现代人工智能（AI）。跟进他们的工作，以追踪当前最佳的 DL 和 ML 研究进展对我们来说也至关重要。 在本论文中，我们首先简述过去的研究论文，对深度学习的模型和方法进行研究。然后，我们将开始描述这一领域的最新进展。我们将讨论深度学习（DL）方法、深度架构（即深度神经网络（DNN））和深度生成模型（DGM），其次是重要的正则化和优化方法。此外，用两个简短的部分对于开源的 DL 框架和重要的 DL 应用进行总结。我们将在最后两个章节（即讨论和结论）中讨论深入学习的现状和未来。 2. 相关研究在过去的几年中，有许多关于深度学习的综述论文。他们以很好的方式描述了 DL 方法、方法论以及它们的应用和未来研究方向。这里，我们简要介绍一些关于深度学习的优秀综述论文。 Young 等人（2017）讨论了 DL 模型和架构，主要用于自然语言处理（NLP）。他们在不同的 NLP 领域中展示了 DL 应用，比较了 DL 模型，并讨论了可能的未来趋势。 Zhang 等人（2017）讨论了用于前端和后端语音识别系统的当前最佳深度学习技术。 Zhu 等人（2017）综述了 DL 遥感技术的最新进展。他们还讨论了开源的 DL 框架和其他深度学习的技术细节。 Wang 等人（2017）以时间顺序的方式描述了深度学习模型的演变。该短文简要介绍了模型，以及在 DL 研究中的突破。该文以进化的方式来了解深度学习的起源，并对神经网络的优化和未来的研究做了解读。 Goodfellow 等人（2016）详细讨论了深度网络和生成模型，从机器学习（ML）基础知识、深度架构的优缺点出发，对近年来的 DL 研究和应用进行了总结。 LeCun 等人（2015）从卷积神经网络（CNN）和递归神经网络（RNN）概述了深度学习（DL）模型。他们从表征学习的角度描述了 DL，展示了 DL 技术如何工作、如何在各种应用中成功使用、以及如何对预测未来进行基于无监督学习（UL）的学习。同时他们还指出了 DL 在文献目录中的主要进展。 Schmidhuber（2015）从 CNN、RNN 和深度强化学习 (RL) 对深度学习做了一个概述。他强调了序列处理的 RNN，同时指出基本 DL 和 NN 的局限性，以及改进它们的技巧。 Nielsen (2015) 用代码和例子描述了神经网络的细节。他还在一定程度上讨论了深度神经网络和深度学习。 Schmidhuber (2014) 讨论了基于时间序列的神经网络、采用机器学习方法进行分类，以及在神经网络中使用深度学习的历史和进展。 Deng 和 Yu (2014) 描述了深度学习类别和技术，以及 DL 在几个领域的应用。 Bengio (2013) 从表征学习的角度简要概述了 DL 算法，即监督和无监督网络、优化和训练模型。他聚焦于深度学习的许多挑战，例如：为更大的模型和数据扩展算法，减少优化困难，设计有效的缩放方法等。 Bengio 等人 (2013) 讨论了表征和特征学习即深度学习。他们从应用、技术和挑战的角度探讨了各种方法和模型。 Deng (2011) 从信息处理及相关领域的角度对深度结构化学习及其架构进行了概述。 Arel 等人 (2010) 简要概述了近年来的 DL 技术。 Bengio (2009) 讨论了深度架构，即人工智能的神经网络和生成模型。 最近所有关于深度学习（DL）的论文都从多个角度讨论了深度学习重点。这对 DL 的研究人员来说是非常有必要的。然而，DL 目前是一个蓬勃发展的领域。在最近的 DL 概述论文发表之后，仍有许多新的技术和架构被提出。此外，以往的论文从不同的角度进行研究。我们的论文主要是针对刚进入这一领域的学习者和新手。为此，我们将努力为新研究人员和任何对这一领域感兴趣的人提供一个深度学习的基础和清晰的概念。 3. 最新进展在本节中，我们将讨论最近从机器学习和人工神经网络 (ANN) 的中衍生出来的主要深度学习 (DL) 方法，人工神经网络是深度学习最常用的形式。 3.1 深度架构的演变人工神经网络 (ANN) 已经取得了长足的进步，同时也带来了其他的深度模型。第一代人工神经网络由简单的感知器神经层组成，只能进行有限的简单计算。第二代使用反向传播，根据错误率更新神经元的权重。然后支持向量机 (SVM) 浮出水面，在一段时间内超越 ANN。为了克服反向传播的局限性，人们提出了受限玻尔兹曼机（RBM），使学习更容易。此时其他技术和神经网络也出现了，如前馈神经网络 (FNN)、卷积神经网络 (CNN)、循环神经网络 (RNN) 等，以及深层信念网络、自编码器等。从那时起，为实现各种用途，ANN 在不同方面得到了改进和设计。 Schmidhuber (2014)、Bengio (2009)、Deng 和 Yu (2014)、Goodfellow 等人 (2016)、Wang 等人 (2017) 对深度神经网络 (DNN) 的进化和历史以及深度学习 (DL) 进行了详细的概述。在大多数情况下，深层架构是简单架构的多层非线性重复，这样可从输入中获得高度复杂的函数。 4. 深度学习方法深度神经网络在监督学习中取得了巨大的成功。此外，深度学习模型在无监督、混合和强化学习方面也非常成功。 4.1 深度监督学习监督学习应用在当数据标记、分类器分类或数值预测的情况。LeCun 等人 (2015) 对监督学习方法以及深层结构的形成给出了一个精简的解释。Deng 和 Yu(2014) 提到了许多用于监督和混合学习的深度网络，并做出解释，例如深度堆栈网络 (DSN) 及其变体。Schmidthuber(2014) 的研究涵盖了所有神经网络，从早期神经网络到最近成功的卷积神经网络 (CNN)、循环神经网络 (RNN)、长短期记忆 (LSTM) 及其改进。 4.2 深度无监督学习当输入数据没有标记时，可应用无监督学习方法从数据中提取特征并对其进行分类或标记。LeCun 等人 (2015) 预测了无监督学习在深度学习中的未来。Schmidthuber(2014) 也描述了无监督学习的神经网络。Deng 和 Yu(2014) 简要介绍了无监督学习的深度架构，并详细解释了深度自编码器。 4.3 深度强化学习强化学习使用奖惩系统预测学习模型的下一步。这主要用于游戏和机器人，解决平常的决策问题。Schmidthuber(2014) 描述了强化学习 (RL) 中深度学习的进展，以及深度前馈神经网络 (FNN) 和循环神经网络 (RNN) 在 RL 中的应用。Li(2017) 讨论了深度强化学习 (Deep Reinforcement Learning, DRL)、它的架构 (例如 Deep Q-Network, DQN) 以及在各个领域的应用。 Mnih 等人 (2016) 提出了一种利用异步梯度下降进行 DNN 优化的 DRL 框架。 van Hasselt 等人 (2015) 提出了一种使用深度神经网络 (deep neural network, DNN) 的 DRL 架构。 5. 深度神经网络在本节中，我们将简要地讨论深度神经网络 (DNN)，以及它们最近的改进和突破。神经网络的功能与人脑相似。它们主要由神经元和连接组成。当我们说深度神经网络时，我们可以假设有相当多的隐藏层，可以用来从输入中提取特征和计算复杂的函数。Bengio(2009) 解释了深度结构的神经网络，如卷积神经网络(CNN)、自编码器 (AE) 等及其变体。Deng 和 Yu(2014) 详细介绍了一些神经网络架构，如 AE 及其变体。Goodfellow 等 (2016) 对深度前馈网络、卷积网络、递归网络及其改进进行了介绍和技巧性讲解。Schmidhuber(2014) 提到了神经网络从早期神经网络到最近成功技术的完整历史。 5.1 深度自编码器自编码器 (AE) 是神经网络 (NN)，其中输出即输入。AE 采用原始输入，编码为压缩表示，然后解码以重建输入。在深度 AE 中，低隐藏层用于编码，高隐藏层用于解码，误差反向传播用于训练.。 5.1.1 变分自编码器变分自动编码器 (VAE) 可以算作解码器。VAE 建立在标准神经网络上，可以通过随机梯度下降训练 (Doersch,2016)。 5.1.2 多层降噪自编码器在早期的自编码器 (AE) 中，编码层的维度比输入层小（窄）。在多层降噪自编码器 (SDAE) 中，编码层比输入层宽 (Deng and Yu, 2014)。 5.1.3 变换自编码器深度自动编码器 (DAE) 可以是转换可变的，也就是从多层非线性处理中提取的特征可以根据学习者的需要而改变。变换自编码器 (TAE) 既可以使用输入向量，也可以使用目标输出向量来应用转换不变性属性，将代码引导到期望的方向 (Deng and Yu,2014)。 5.2 深度卷积神经网络四种基本思想构成了卷积神经网络 (CNN)，即：局部连接、共享权重、池化和多层使用。CNN 的第一部分由卷积层和池化层组成，后一部分主要是全连接层。卷积层检测特征的局部连接，池层将相似的特征合并为一个。CNN 在卷积层中使用卷积而不是矩阵乘法。 Krizhevsky 等人 (2012) 提出了一种深度卷积神经网络 (CNN) 架构，也称为 AlexNet，这是深度学习 (Deep Learning, DL) 的一个重大突破。网络由 5 个卷积层和 3 个全连接层组成。该架构采用图形处理单元 (GPU) 进行卷积运算，采用线性整流函数 (ReLU) 作为激活函数，用 Dropout 来减少过拟合。 Iandola 等人 (2016) 提出了一个小型的 CNN 架构，叫做「SqueezeNet」。 Szegedy 等人 (2014) 提出了一种深度 CNN 架构，名为 Inception。Dai 等人 (2017) 提出了对 Inception-ResNet 的改进。 Redmon 等人 (2015) 提出了一个名为 YOLO (You Only Look Once) 的 CNN 架构，用于均匀和实时的目标检测。 Zeiler 和 Fergus (2013) 提出了一种将 CNN 内部激活可视化的方法。 Gehring 等人 (2017) 提出了一种用于序列到序列学习的 CNN 架构。 Bansal 等人 (2017) 提出了 PixelNet，使用像素来表示。 Goodfellow 等人 (2016) 解释了 CNN 的基本架构和思想。Gu 等人 (2015) 对 CNN 的最新进展、CNN 的多种变体、CNN 的架构、正则化方法和功能以及在各个领域的应用进行了很好的概述。 5.2.1 深度最大池化卷积神经网络最大池化卷积神经网络 (MPCNN) 主要对卷积和最大池化进行操作，特别是在数字图像处理中。MPCNN 通常由输入层以外的三种层组成。卷积层获取输入图像并生成特征图，然后应用非线性激活函数。最大池层向下采样图像，并保持子区域的最大值。全连接层进行线性乘法。在深度 MPCNN 中，在输入层之后周期性地使用卷积和混合池化，然后是全连接层。 5.2.2 极深的卷积神经网络Simonyan 和 Zisserman(2014) 提出了非常深层的卷积神经网络 (VDCNN) 架构，也称为 VGG Net。VGG Net 使用非常小的卷积滤波器，深度达到 16-19 层。Conneau 等人 (2016) 提出了另一种文本分类的 VDCNN 架构，使用小卷积和池化。他们声称这个 VDCNN 架构是第一个在文本处理中使用的，它在字符级别上起作用。该架构由 29 个卷积层组成。 5.3 网络中的网络Lin 等人 (2013) 提出了网络中的网络 (Network In Network,NIN)。NIN 以具有复杂结构的微神经网络代替传统卷积神经网络 (CNN) 的卷积层。它使用多层感知器 (MLPConv) 处理微神经网络和全局平均池化层，而不是全连接层。深度 NIN 架构可以由 NIN 结构的多重叠加组成。 5.4 基于区域的卷积神经网络Girshick 等人 (2014) 提出了基于区域的卷积神经网络 (R-CNN)，使用区域进行识别。R-CNN 使用区域来定位和分割目标。该架构由三个模块组成：定义了候选区域的集合的类别独立区域建议，从区域中提取特征的大型卷积神经网络 (CNN)，以及一组类特定的线性支持向量机 (SVM)。 5.4.1 Fast R-CNNGirshick(2015) 提出了快速的基于区域的卷积网络 (Fast R-CNN)。这种方法利用 R-CNN 架构能快速地生成结果。Fast R-CNN 由卷积层和池化层、区域建议层和一系列全连接层组成。 5.4.2 Faster R-CNNRen 等人 (2015) 提出了更快的基于区域的卷积神经网络 (Faster R-CNN)，它使用区域建议网络 (Region Proposal Network, RPN) 进行实时目标检测。RPN 是一个全卷积网络，能够准确、高效地生成区域建议 (Ren et al.，2015)。 5.4.3 Mask R-CNN何恺明等人 (2017) 提出了基于区域的掩模卷积网络 (Mask R-CNN) 实例目标分割。Mask R-CNN 扩展了 R-CNN 的架构，并使用一个额外的分支用于预测目标掩模。 5.4.4 Multi-Expert R-CNN.Lee 等人 (2017) 提出了基于区域的多专家卷积神经网络 (ME R-CNN)，利用了 Fast R-CNN 架构。ME R-CNN 从选择性和详尽的搜索中生成兴趣区域 (RoI)。它也使用 per-RoI 多专家网络而不是单一的 per-RoI 网络。每个专家都是来自 Fast R-CNN 的全连接层的相同架构。 5.5 深度残差网络He 等人 (2015) 提出的残差网络 (ResNet) 由 152 层组成。ResNet 具有较低的误差，并且容易通过残差学习进行训练。更深层次的 ResNet 可以获得更好的性能。在深度学习领域，人们认为 ResNet 是一个重要的进步。 5.5.1 Resnet in ResnetTarg 等人 (2016) 在 Resnet in Resnet (RiR) 中提出将 ResNets 和标准卷积神经网络 (CNN) 结合到深层双流架构中。 5.5.2 ResNeXtXie 等人 (2016) 提出了 ResNeXt 架构。ResNext 利用 ResNets 来重复使用分割-转换-合并策略。 5.6 胶囊网络Sabour 等人 (2017) 提出了胶囊网络 (CapsNet)，即一个包含两个卷积层和一个全连接层的架构。CapsNet 通常包含多个卷积层，胶囊层位于末端。CapsNet 被认为是深度学习的最新突破之一，因为据说这是基于卷积神经网络的局限性而提出的。它使用的是一层又一层的胶囊，而不是神经元。激活的较低级胶囊做出预测，在同意多个预测后，更高级的胶囊变得活跃。在这些胶囊层中使用了一种协议路由机制。Hinton 之后提出 EM 路由，利用期望最大化 (EM) 算法对 CapsNet 进行了改进。 5.7 循环神经网络循环神经网络 (RNN) 更适合于序列输入，如语音、文本和生成序列。一个重复的隐藏单元在时间展开时可以被认为是具有相同权重的非常深的前馈网络。由于梯度消失和维度爆炸问题，RNN 曾经很难训练。为了解决这个问题，后来许多人提出了改进意见。 Goodfellow 等人 (2016) 详细分析了循环和递归神经网络和架构的细节，以及相关的门控和记忆网络。 Karpathy 等人 (2015) 使用字符级语言模型来分析和可视化预测、表征训练动态、RNN 及其变体 (如 LSTM) 的错误类型等。 J´ozefowicz 等人 (2016) 探讨了 RNN 模型和语言模型的局限性。 5.7.1 RNN-EMPeng 和 Yao(2015) 提出了利用外部记忆 (RNN-EM) 来改善 RNN 的记忆能力。他们声称在语言理解方面达到了最先进的水平，比其他 RNN 更好。 5.7.2 GF-RNNChung 等 (2015) 提出了门控反馈递归神经网络 (GF-RNN)，它通过将多个递归层与全局门控单元叠加来扩展标准的 RNN。 5.7.3 CRF-RNNZheng 等人 (2015) 提出条件随机场作为循环神经网络 (CRF-RNN)，其将卷积神经网络 (CNN) 和条件随机场 (CRF) 结合起来进行概率图形建模。 5.7.4 Quasi-RNNBradbury 等人 (2016) 提出了用于神经序列建模和沿时间步的并行应用的准循环神经网络 (QRNN)。 5.8 记忆网络Weston 等人 (2014) 提出了问答记忆网络 (QA)。记忆网络由记忆、输入特征映射、泛化、输出特征映射和响应组成。 5.8.1 动态记忆网络Kumar 等人 (2015) 提出了用于 QA 任务的动态记忆网络 (DMN)。DMN 有四个模块:输入、问题、情景记忆、输出。 5.9 增强神经网络Olah 和 Carter(2016) 很好地展示了注意力和增强循环神经网络，即神经图灵机 (NTM)、注意力接口、神经编码器和自适应计算时间。增强神经网络通常是使用额外的属性，如逻辑函数以及标准的神经网络架构。 5.9.1 神经图灵机Graves 等人 (2014) 提出了神经图灵机 (NTM) 架构，由神经网络控制器和记忆库组成。NTM 通常将 RNN 与外部记忆库结合。 5.9.2 神经 GPUKaiser 和 Sutskever(2015) 提出了神经 GPU，解决了 NTM 的并行问题。 5.9.3 神经随机存取机Kurach 等人 (2015) 提出了神经随机存取机，它使用外部的可变大小的随机存取存储器。 5.9.4 神经编程器Neelakantan 等人 (2015) 提出了神经编程器，一种具有算术和逻辑功能的增强神经网络。 5.9.5 神经编程器-解释器 Reed 和 de Freitas(2015) 提出了可以学习的神经编程器-解释器 (NPI)。NPI 包括周期性内核、程序内存和特定于领域的编码器。 5.10 长短期记忆网络Hochreiter 和 Schmidhuber(1997) 提出了长短期记忆 (Long short - Short-Term Memory, LSTM)，克服了循环神经网络 (RNN) 的误差回流问题。LSTM 是基于循环网络和基于梯度的学习算法，LSTM 引入自循环产生路径，使得梯度能够流动。 Greff 等人 (2017) 对标准 LSTM 和 8 个 LSTM 变体进行了大规模分析，分别用于语音识别、手写识别和复调音乐建模。他们声称 LSTM 的 8 个变种没有显著改善，而只有标准 LSTM 表现良好。 Shi 等人 (2016b) 提出了深度长短期记忆网络 (DLSTM)，它是一个 LSTM 单元的堆栈，用于特征映射学习表示。 5.10.1 批-归一化 LSTMCooijmans 等人 (2016) 提出了批-归一化 LSTM (BN-LSTM)，它对递归神经网络的隐藏状态使用批-归一化。 5.10.2 Pixel RNNvan den Oord 等人 (2016b) 提出像素递归神经网络 (Pixel-RNN)，由 12 个二维 LSTM 层组成。 5.10.3 双向 LSTM W¨ollmer 等人 (2010) 提出了双向 LSTM(BLSTM) 的循环网络与动态贝叶斯网络 (DBN) 一起用于上下文敏感关键字检测。 5.10.4 Variational Bi-LSTMShabanian 等人 (2017) 提出了变分双向 LSTM（Variational Bi-LSTM），它是双向 LSTM 体系结构的变体。Variational Bi-LSTM 使用变分自编码器 (VAE) 在 LSTM 之间创建一个信息交换通道，以学习更好的表征。 5.11 谷歌神经机器翻译Wu 等人 (2016) 提出了名为谷歌神经机器翻译 (GNMT) 的自动翻译系统，该系统结合了编码器网络、解码器网络和注意力网络，遵循共同的序列对序列 (sequence-to-sequence) 的学习框架。 5.12 Fader NetworkLample 等人 (2017) 提出了 Fader 网络，这是一种新型的编码器-解码器架构，通过改变属性值来生成真实的输入图像变化。 5.13 超网络Ha 等人 (2016) 提出的超网络（Hyper Networks）为其他神经网络生成权值，如静态超网络卷积网络、用于循环网络的动态超网络。 Deutsch(2018) 使用超网络生成神经网络。 5.14 Highway NetworksSrivastava 等人 (2015) 提出了高速路网络（Highway Networks），通过使用门控单元来学习管理信息。跨多个层次的信息流称为信息高速路。 5.14.1 Recurrent Highway NetworksZilly 等人 (2017) 提出了循环高速路网络 (Recurrent Highway Networks，RHN)，它扩展了长短期记忆 (LSTM) 架构。RHN 在周期性过渡中使用了 Highway 层。 5.15 Highway LSTM RNNZhang 等人 (2016) 提出了高速路长短期记忆 (high - Long short Memory, HLSTM) RNN，它在相邻层的内存单元之间扩展了具有封闭方向连接 (即 Highway) 的深度 LSTM 网络。 5.16 长期循环 CNNDonahue 等人 (2014) 提出了长期循环卷积网络 (LRCN)，它使用 CNN 进行输入，然后使用 LSTM 进行递归序列建模并生成预测。 5.17 深度神经 SVMZhang 等人 (2015) 提出了深度神经 SVM(DNSVM)，它以支持向量机 (Support Vector Machine, SVM) 作为深度神经网络 (Deep Neural Network, DNN) 分类的顶层。 5.18 卷积残差记忆网络Moniz 和 Pal(2016) 提出了卷积残差记忆网络，将记忆机制并入卷积神经网络 (CNN)。它用一个长短期记忆机制来增强卷积残差网络。 5.19 分形网络Larsson 等人 (2016) 提出分形网络即 FractalNet 作为残差网络的替代方案。他们声称可以训练超深度的神经网络而不需要残差学习。分形是简单扩展规则生成的重复架构。 5.20 WaveNetvan den Oord 等人 (2016) 提出了用于产生原始音频的深度神经网络 WaveNet。WaveNet 由一堆卷积层和 softmax 分布层组成，用于输出。 Rethage 等人 (2017) 提出了一个 WaveNet 模型用于语音去噪。 5.21 指针网络 Vinyals 等人 (2017) 提出了指针网络 (Ptr-Nets)，通过使用一种称为「指针」的 softmax 概率分布来解决表征变量字典的问题。 6. 深度生成模型在本节中，我们将简要讨论其他深度架构，它们使用与深度神经网络类似的多个抽象层和表示层，也称为深度生成模型 (deep generate Models, DGM)。Bengio(2009) 解释了深层架构，例如 Boltzmann machine(BM) 和 Restricted Boltzmann Machines (RBM) 等及其变体。 Goodfellow 等人 (2016) 详细解释了深度生成模型，如受限和非受限的玻尔兹曼机及其变种、深度玻尔兹曼机、深度信念网络 (DBN)、定向生成网络和生成随机网络等。 Maaløe 等人（2016）提出了辅助的深层生成模型（Auxiliary Deep Generative Models），在这些模型中，他们扩展了具有辅助变量的深层生成模型。辅助变量利用随机层和跳过连接生成变分分布。 Rezende 等人 (2016) 开发了一种深度生成模型的单次泛化。 6.1 玻尔兹曼机玻尔兹曼机是学习任意概率分布的连接主义方法，使用最大似然原则进行学习。 6.2 受限玻尔兹曼机受限玻尔兹曼机 (Restricted Boltzmann Machines, RBM) 是马尔可夫随机场的一种特殊类型，包含一层随机隐藏单元，即潜变量和一层可观测变量。 Hinton 和 Salakhutdinov(2011) 提出了一种利用受限玻尔兹曼机 (RBM) 进行文档处理的深度生成模型。 6.3 深度信念网络深度信念网络 (Deep Belief Networks, DBN) 是具有多个潜在二元或真实变量层的生成模型。 Ranzato 等人 (2011) 利用深度信念网络 (deep Belief Network, DBN) 建立了深度生成模型进行图像识别。 6.4 深度朗伯网络Tang 等人 (2012) 提出了深度朗伯网络 (Deep Lambertian Networks，DLN)，它是一个多层次的生成模型，其中潜在的变量是反照率、表面法线和光源。DLNis 是朗伯反射率与高斯受限玻尔兹曼机和深度信念网络的结合。 6.5 生成对抗网络Goodfellow 等人 (2014) 提出了生成对抗网络 (generate Adversarial Nets, GAN)，用于通过对抗过程来评估生成模型。GAN 架构是由一个针对对手（即一个学习模型或数据分布的判别模型）的生成模型组成。Mao 等人 (2016)、Kim 等人 (2017) 对 GAN 提出了更多的改进。 Salimans 等人 (2016) 提出了几种训练 GANs 的方法。 6.5.1 拉普拉斯生成对抗网络Denton 等人 (2015) 提出了一种深度生成模型 (DGM)，叫做拉普拉斯生成对抗网络 (LAPGAN)，使用生成对抗网络 (GAN) 方法。该模型还在拉普拉斯金字塔框架中使用卷积网络。 6.6 循环支持向量机Shi 等人 (2016a) 提出了循环支持向量机 (RSVM)，利用循环神经网络 (RNN) 从输入序列中提取特征，用标准支持向量机 (SVM) 进行序列级目标识别。 7. 训练和优化技术在本节中，我们将简要概述一些主要的技术，用于正则化和优化深度神经网络 (DNN)。 7.1 DropoutSrivastava 等人 (2014) 提出 Dropout，以防止神经网络过拟合。Dropout 是一种神经网络模型平均正则化方法，通过增加噪声到其隐藏单元。在训练过程中，它会从神经网络中随机抽取出单元和连接。Dropout 可以用于像 RBM (Srivastava et al.，2014) 这样的图形模型中，也可以用于任何类型的神经网络。最近提出的一个关于 Dropout 的改进是 Fraternal Dropout，用于循环神经网络 (RNN)。 7.2 MaxoutGoodfellow 等人 (2013) 提出 Maxout，一种新的激活函数，用于 Dropout。Maxout 的输出是一组输入的最大值，有利于 Dropout 的模型平均。 7.3 ZoneoutKrueger 等人 (2016) 提出了循环神经网络 (RNN) 的正则化方法 Zoneout。Zoneout 在训练中随机使用噪音，类似于 Dropout，但保留了隐藏的单元而不是丢弃。 7.4 深度残差学习He 等人 (2015) 提出了深度残差学习框架，该框架被称为低训练误差的 ResNet。 7.5 批归一化 Ioffe 和 Szegedy(2015) 提出了批归一化，通过减少内部协变量移位来加速深度神经网络训练的方法。Ioffe(2017) 提出批重归一化，扩展了以前的方法。 7.6 DistillationHinton 等人 (2015) 提出了将知识从高度正则化模型的集合 (即神经网络) 转化为压缩小模型的方法。 7.7 层归一化 Ba 等人 (2016) 提出了层归一化，特别是针对 RNN 的深度神经网络加速训练，解决了批归一化的局限性。 8. 深度学习框架有大量的开源库和框架可供深度学习使用。它们大多数是为 Python 编程语言构建的。如 Theano、Tensorflow、PyTorch、PyBrain、Caffe、Blocks and Fuel 、CuDNN、Honk、ChainerCV、PyLearn2、Chainer,、torch 等。 9. 深度学习的应用在本节中，我们将简要地讨论一些最近在深度学习方面的杰出应用。自深度学习 (DL) 开始以来，DL 方法以监督、非监督、半监督或强化学习的形式被广泛应用于各个领域。从分类和检测任务开始，DL 应用正在迅速扩展到每一个领域。 例如： 图像分类与识别 视频分类 序列生成 缺陷分类 文本、语音、图像和视频处理 文本分类 语音处理 语音识别和口语理解 文本到语音生成 查询分类 句子分类 句子建模 词汇处理 预选择 文档和句子处理 生成图像文字说明 照片风格迁移 自然图像流形 图像着色 图像问答 生成纹理和风格化图像 视觉和文本问答 视觉识别和描述 目标识别 文档处理 人物动作合成和编辑 歌曲合成 身份识别 人脸识别和验证 视频动作识别 人类动作识别 动作识别 分类和可视化动作捕捉序列 手写生成和预测 自动化和机器翻译 命名实体识别 移动视觉 对话智能体 调用遗传变异 癌症检测 X 射线 CT 重建 癫痫发作预测 硬件加速 机器人 等。 Deng 和 Yu(2014) 提供了 DL 在语音处理、信息检索、目标识别、计算机视觉、多模态、多任务学习等领域应用的详细列表。 使用深度强化学习 (Deep Reinforcement Learning, DRL) 来掌握游戏已经成为当今的一个热门话题。每到现在，人工智能机器人都是用 DNN 和 DRL 创建的，它们在战略和其他游戏中击败了人类世界冠军和象棋大师，从几个小时的训练开始。例如围棋的 AlphaGo 和 AlphaGo Zero。 10. 讨论尽管深度学习在许多领域取得了巨大的成功，但它还有很长的路要走。还有很多地方有待改进。至于局限性，例子也是相当多的。例如：Nguyen 等人表明深度神经网络（DNN）在识别图像时容易被欺骗。还有其他问题，如 Yosinski 等人提出的学习的特征可迁移性。Huang 等人提出了一种神经网络攻击防御的体系结构，认为未来的工作需要防御这些攻击。Zhang 等人则提出了一个理解深度学习模型的实验框架，他们认为理解深度学习需要重新思考和概括。 Marcus 在 2018 年对深度学习 (Deep Learning, DL) 的作用、局限性和本质进行了重要的回顾。他强烈指出了 DL 方法的局限性，即需要更多的数据，容量有限，不能处理层次结构，无法进行开放式推理，不能充分透明，不能与先验知识集成，不能区分因果关系。他还提到，DL 假设了一个稳定的世界，以近似方法实现，工程化很困难，并且存在着过度炒作的潜在风险。Marcus 认为 DL 需要重新概念化，并在非监督学习、符号操作和混合模型中寻找可能性，从认知科学和心理学中获得见解，并迎接更大胆的挑战。 11. 结论尽管深度学习（DL）比以往任何时候都更快地推进了世界的发展，但仍有许多方面值得我们去研究。我们仍然无法完全地理解深度学习，我们如何让机器变得更聪明，更接近或比人类更聪明，或者像人类一样学习。DL 一直在解决许多问题，同时将技术应用到方方面面。但是人类仍然面临着许多难题，例如仍有人死于饥饿和粮食危机, 癌症和其他致命的疾病等。我们希望深度学习和人工智能将更加致力于改善人类的生活质量，通过开展最困难的科学研究。最后但也是最重要的，愿我们的世界变得更加美好。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://wengjj.ink/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"论文翻译","slug":"深度学习/论文翻译","permalink":"https://wengjj.ink/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://wengjj.ink/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"100-Days-Of-ML-Code 100天机器学习挑战","slug":"100-Days-Of-ML-Code 100天机器学习挑战","date":"2018-08-08T08:42:09.000Z","updated":"2020-08-30T08:02:31.736Z","comments":true,"path":"2018/08/08/100-Days-Of-ML-Code 100天机器学习挑战/","link":"","permalink":"https://wengjj.ink/2018/08/08/100-Days-Of-ML-Code%20100%E5%A4%A9%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8C%91%E6%88%98/","excerpt":"","text":"ML网红Siraj Raval发起了一个名为：100-Days-Of-ML-Code的挑战赛，意为鼓励同学们每天抽出点时间来学习机器学习，看看100天后你有了哪些进步目前Github上面最火爆的100-Days-Of-ML-Code就是Avik-Jain的机器学习项目，超赞的配图，清晰的知识点梳理，是入门机器学习非常好的项目** Github:https://github.com/Avik-Jain/100-Days-Of-ML-Code 目前和一群小伙伴已得到Avik-Jain的授权，正在着手对Avik-Jain/100-Days-Of-ML-Code这个项目进行中文版本的编译，不单单只是编译，我们还会在原项目上提交完整的jupyter代码 Github:https://github.com/MachineLearning100/100-Days-Of-ML-Code 这些对正在入门机器学习的同学们是非常好的入门教材，欢迎大家给原作者Avik-Jain和我们中文小组点Star！ 项目地址：100-Days-Of-ML-Code","categories":[{"name":"比赛项目","slug":"比赛项目","permalink":"https://wengjj.ink/categories/%E6%AF%94%E8%B5%9B%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"100天机器学习挑战","slug":"100天机器学习挑战","permalink":"https://wengjj.ink/tags/100%E5%A4%A9%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8C%91%E6%88%98/"}]},{"title":"数据矿工学习-《统计学习方法》思维导图7.3-非线性支持向量机与核函数","slug":"数据矿工学习-《统计学习方法》思维导图7.3-非线性支持向量机与核函数","date":"2018-08-02T06:33:37.000Z","updated":"2020-08-30T08:27:15.730Z","comments":true,"path":"2018/08/02/数据矿工学习-《统计学习方法》思维导图7.3-非线性支持向量机与核函数/","link":"","permalink":"https://wengjj.ink/2018/08/02/%E6%95%B0%E6%8D%AE%E7%9F%BF%E5%B7%A5%E5%AD%A6%E4%B9%A0-%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE7.3-%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E4%B8%8E%E6%A0%B8%E5%87%BD%E6%95%B0/","excerpt":"前两篇我们看的是都是适应线性样本的支持向量机，那遇到非线性的分类问题呢？利用核技巧，就可以将线性分类的学习方法应用到非线性分类问题中去，将线性支持向量机拓展到非线性支持向量机，只需将线性支持向量机对偶形式中的内积换成核函数，接下来就来看下非线性支持向量机的思维导图：","text":"前两篇我们看的是都是适应线性样本的支持向量机，那遇到非线性的分类问题呢？利用核技巧，就可以将线性分类的学习方法应用到非线性分类问题中去，将线性支持向量机拓展到非线性支持向量机，只需将线性支持向量机对偶形式中的内积换成核函数，接下来就来看下非线性支持向量机的思维导图： 思维来自《统计学习方法》-李航 凹脑图在线浏览地址：非线性支持向量机才学疏浅，欢迎评论指导","categories":[{"name":"《统计学习方法-李航》","slug":"《统计学习方法-李航》","permalink":"https://wengjj.ink/categories/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9D%8E%E8%88%AA%E3%80%8B/"}],"tags":[{"name":"非线性支持向量机","slug":"非线性支持向量机","permalink":"https://wengjj.ink/tags/%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"},{"name":"核函数","slug":"核函数","permalink":"https://wengjj.ink/tags/%E6%A0%B8%E5%87%BD%E6%95%B0/"}]},{"title":"数据矿工学习-《统计学习方法》思维导图7.2-线性支持向量机","slug":"数据矿工学习-《统计学习方法》思维导图7.2-线性支持向量机","date":"2018-07-24T06:25:39.000Z","updated":"2020-08-30T08:02:00.073Z","comments":true,"path":"2018/07/24/数据矿工学习-《统计学习方法》思维导图7.2-线性支持向量机/","link":"","permalink":"https://wengjj.ink/2018/07/24/%E6%95%B0%E6%8D%AE%E7%9F%BF%E5%B7%A5%E5%AD%A6%E4%B9%A0-%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE7.2-%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/","excerpt":"上一章讲了线性可分支持向量机，但在实际工程中，样本数据往往是数据不可分的，此时就需要另一种学习器-线性支持向量机，又称线性不可分支持向量机，因为其适用范围更广，所以一般将其称为线性支持向量机，其学习策略为-软间隔最大化（区别于线性可分支持向量机的硬间隔最大化）：","text":"上一章讲了线性可分支持向量机，但在实际工程中，样本数据往往是数据不可分的，此时就需要另一种学习器-线性支持向量机，又称线性不可分支持向量机，因为其适用范围更广，所以一般将其称为线性支持向量机，其学习策略为-软间隔最大化（区别于线性可分支持向量机的硬间隔最大化）： 思维来自《统计学习方法》-李航 凹脑图在线浏览地址：线性支持向量机才学疏浅，欢迎评论指导","categories":[{"name":"《统计学习方法-李航》","slug":"《统计学习方法-李航》","permalink":"https://wengjj.ink/categories/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9D%8E%E8%88%AA%E3%80%8B/"}],"tags":[{"name":"线性支持向量机","slug":"线性支持向量机","permalink":"https://wengjj.ink/tags/%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"}]},{"title":"数据矿工学习-样本自适应的在线卷积稀疏编码论文-个人中文翻译","slug":"数据矿工学习-样本自适应的在线卷积稀疏编码论文-个人中文翻译","date":"2018-07-23T08:47:37.000Z","updated":"2020-08-30T08:46:23.196Z","comments":true,"path":"2018/07/23/数据矿工学习-样本自适应的在线卷积稀疏编码论文-个人中文翻译/","link":"","permalink":"https://wengjj.ink/2018/07/23/%E6%95%B0%E6%8D%AE%E7%9F%BF%E5%B7%A5%E5%AD%A6%E4%B9%A0-%E6%A0%B7%E6%9C%AC%E8%87%AA%E9%80%82%E5%BA%94%E7%9A%84%E5%9C%A8%E7%BA%BF%E5%8D%B7%E7%A7%AF%E7%A8%80%E7%96%8F%E7%BC%96%E7%A0%81%E8%AE%BA%E6%96%87-%E4%B8%AA%E4%BA%BA%E4%B8%AD%E6%96%87%E7%BF%BB%E8%AF%91/","excerpt":"论文地址 ： https://arxiv.org/abs/1804.10366 文中的数学公式符号并不能很好的显示，采用普通字母代替，故带有一定的误差，建议数学公式的推导还是回归论文查看。","text":"论文地址 ： https://arxiv.org/abs/1804.10366 文中的数学公式符号并不能很好的显示，采用普通字母代替，故带有一定的误差，建议数学公式的推导还是回归论文查看。 简介卷积稀疏编码（CSC）已被广泛用于图像和信号处理中的平移不变字典的学习。然而，现有的方法的可扩展性是有限的。在本文中，我们使用的不是样本共享的字典来卷积，而是使用样本自适应字典，其中每个过滤器是从数据中学习的一组基本滤波器的线性组合。这种灵活性允许捕获大量依赖于样本的模式，这在处理大型或高维数据集时特别有用。在计算上，所得到的模型可以通过在线学习来进行有效地学习。在大型数据集上的实验结果表明，所提出的方法优于现有的CSC算法，可以显著减少的时间和空间复杂度。 1.序言卷积稀疏编码目前已成功地应用在图像处理、信号处理和生物医学应用中。它与稀疏编码密切相关，但CSC的优点在于它的平移不变字典可以捕获信号和图像中常见的偏移局部模式。然后，每个数据样本由对应的代码卷积的字典中的一组滤波器的总和来表示。 传统的CSC算法在批处理模式下运行，采用O（NK^2P+NKPlog p）时间和O（NKP）空间（其中*n是样本数），k是滤波器的数目，p是维数*。近年来，随着数据样本到达一定程度，样本的相关信息被压缩为小型的历史统计数据，并且模型被增量更新，已经提出了许多在线CSC算法拥有了更好的可扩展性。值得一提的是，最先进的OCSC算法具有更小的时间复杂度O（K^2P+KPlog P）和空间复杂度O（K^2P）。 然而，OCSC的复杂性仍然依赖于K，并且不能与大量的过滤器一起使用。因此，可以捕获的局部图案的数量是有限的，并且可能导致性能过低的情况出现，尤其是在高维数据集上更为严重。此外，使用更多的滤波器也导致了大量昂贵的卷积运算。RigaMunTi和SiRONI提出用可分离滤波器来近似地对学习过的滤波器进行后处理，使得卷积变得相对不那么昂贵。然而，由于学习和后处理是两个独立的过程，因此得到的滤波器可能不是最优的。此外，随着新样本的到来，这些独立的过滤器无法在线更新。 CSC拓展的另一个方向是通过分布式计算（BotSekas＆TsisiKLIS，1997）。将数据和工作量分配到多台机器上，最近的共识CSC算法（ChoudHury等人，2017）可以处理更大型、更高维的数据集，例如视频、多光谱图像和光场图像。然而，关于CSC的计算需求只是通过共享运算平台解决，并没有真正地解决CSC的计算量问题。 在本篇论文中，我们提出了通过从数据中学习的一组小型基础滤波器的自适应组合来近似大型滤波器。虽然标准CSC字典由所有样本共享，但我们建议每个样本都有自己的“个人”字典来补偿使用这些在线卷积稀疏编码与样本自适应基滤波器而导致的灵活性降低的问题。以这种方式，表现效果可以保持相同，但参数数目却可以有效减少。计算上，这种结构还允许开发高效的在线学习算法。具体地，基础滤波器可以通过乘法器交替方向法（ADMM）更新（博伊德等人，2011），而代码和组合权重可以通过加速的近端算法来学习（姚等人，2017）。在各种数据集上的广泛的实验结果表明，这种SCSC算法在时间和空间上都具有不错的效果，并且优于现有的批处理、在线和分布式CSC算法。 论文的其余部分按如下方式组织。第2节简要回顾卷积稀疏编码。第3节描述了所提出的算法。实验结果在第4节中给出，最后一节给出了一些相关结论。 2.卷积稀疏编码综述 Review: Convolutional Sparse Coding给定的样本{x1，……，xn}在R^P中，CSC学习一个*移位不变字典**D∈R^M×K，其中列{*D(: , k)} 表示k个滤波器。每个样本xi都被编码为Zi∈R^P×K，kth列是用滤波器D（:，k）卷积的代码。字典和代码是通过解决最优化问题一起学习得到的： 当第一项测量信号出现重构误差时，D = {D :||D(:,k) ||2 ≤ 1, ∀k = 1, . . . , K}保证了滤波器的归一化，β＞0是控制Zi’s稀疏性的正则化参数。 在空间域中执行（1）中的卷积。这需要O（KPM）时间，而且价格昂贵。相比之下，最近的CSC方法在频域上进行卷积，其取O（KPlog p）时间（Mallat，1999），并且对于M和P的典型选择更快，让x˜i ≡ *F*(xi)，D˜(:,k) ≡ F(D(:,k))和Z˜ i(:,k) ≡ F(Zi(:,k))成为xi ,D(:,k) and Zi(:,k)傅立叶变换后的对应。代码和滤波器以块坐标下降交替方式更新，如下： （2）和（3）都可以用乘法器交替方向法（ADMM）来求解（博伊德等人，2011）。随后，{Z˜ i}和D˜可以被转换回空间域，如Zi(:,k) = F^-1 (Z˜ i(:,k)) 和D(:,k)＝C(F^-1(D˜(:,k)))。 注意，虽然Zi’s（在空域中）是稀疏的，但是FFT变换后的的Z˜i’s 不是。 在推理上，给定学习字典D，测试样本xij被重建为 ∑K/k=1 D(:, k) ∗ Zj (:, k)，其中Zj是得到的编码。 2.1.可分离滤波器的后处理 Post-Processing for Separable Filters由CSC获得的滤波器是不可分离的，随后的卷积可能是缓慢的。为了提高速度，它们可以通过可分离滤波器进行后处理和近似（RigaMaTi等人，2013；SiRONI等人，2015）。具体而言，学习的D是由SW近似的，其中S ∈ R^M×R包含R rank-1基础滤波器{S（：，1），…，S（：，R）}和W ∈ R^R×K包含组合权重。然而，这常常会导致性能低下。 2.2. 在线CSC Online CSC在线CSC算法（OCSC）是最近提出的。在最后一次迭代中，给出了傅立叶变换的样本x˜t和字典D˜ t−1，得到了相应的{Z˜t, Ut}，如在（2）中得到的。下面的命题通过重新编写（3）使用较小的历史统计数据来更新D˜t 和Vt。 命题1：Vt可以通过求解优化问题得到： 问题（4）可以用ADMM来解决。总的空间复杂度仅为O（K^2P），与N无关。此外，Ht和Gt可以进行增量更新。 最近还提出了另外两个在线CSC的重新制定方案。Degraux提出的CSC方案通过在空间域上进行卷积，但速度较慢。Liu提出的CSC方案在频域中进行卷积，但需要昂贵的稀疏矩阵运算。 3.具有自适应字典的在线CSC Online CSC with Sample-Dependent Dictionary虽然OCSC在样本大小N的情况下可以很好地扩展，但它的空间复杂度仍然取决于滤波器数量K的平方。 这限制了可以使用的过滤器数量，并且可能影响性能。 受2.1节中可分离滤波器的思想的启发，我们通过用R基础滤波器来近似K滤波器以实现更多滤波器的学习，其中R&lt;K。 与通过后处理获得的可分离滤波器相比，我们建议在信号重建期间直接学习字典。此外，字典中的滤波器以样本自适应的方式从基本滤波器组合。 3.1. 问题描述 Problem Formulation回想一下，每一个xi in（1）都是由∑K/k=1 D(:, k) ∗ Zj (:, k)表示。设t B∈R^M×R，其中列{B(:,r)}是基础滤波器。我们建议将xi表示为： 这是依赖于样本的。如图所示，这允许Wi’进行独立学习（第3.3节）。这也导致更多的自适应模式被捕获，从而获得更好的性能（第4.4节）。 卷积神经网络（CNN）最近研究了样本相关滤波器（Ja等人，2016）。从经验上看，这优于标准CNNs在单镜头学习，视频预测和图像去模糊这三个方面的表现。Jia使用专门设计的神经网络学习滤波器，不考虑CSC模型，而是将样本相关滤波器集成到CSC中。 该词典还可以通过微调来适应单个样本。然而，当K很大时，学习初始共享字典仍然很昂贵。此外，如将在第4.2节中所示，所提出的方法将优于经验的微调。 3.2. 学习方法 Learning将（6）代入到CSC公式中（1），我们得到 由于B和Wi在（8）中耦合在一起，这使得优化问题变得困难。下面的命题将B和Wi解耦。所有的证明都在附录中。 为了简化符号，我们用W表示W’1或W’2。通过在{Wi}上施加上述结构中的任一个，得到以下的优化问题： 在对样本xj进行推算时，可以通过解决(10)来获得相对应的（Wj，Zj）。 3.3.在线学习算法解决（10）优化问题 Online Learning Algorithm for (10)正如在第2.2节中，我们提出了一个更好的可扩展性的在线算法。在tth迭代中，考虑 设B˜(:, r) ≡ F(B(:, r))，其中B(:, r) ∈ R^M使用零填充为P维。 注意，通过重写上面的求和∑R/r=1 B(:, r) ∗( ∑K/k=1 Wi(r, k)Zi(: , k))，可以将卷积的数量从k减少到r。 下面的命题重写（11）并在频域中进行卷积。命题3:问题(11)重写为： 空域基础滤波器可以将B˜还原为B(:,r) = C(F^-1 (B˜(:,r)))。 3.3.1. B˜ t的获取OBTAINING B˜ t 从优化问题（12），可以通过求解子问题来获得B˜ t： 其中V˜是辅助变量，这与（3）的形式相同。因此，类似于（4），B˜t可以通过下方优化问题得到： 3.3.2.得到Wt和Zt OBTAINING Wt AND Zt 随着XT的到来，我们将基础滤波器固定到在最后一次迭代中学习的B˜t−1，并从优化问题（12）获得（Wt，Zt）： 在CSC文献中，可以看出ADMM也可以用于求解（16）。虽然（2）中CSC的代码更新子问题是凸的，但问题（16）是非凸的，并且对于ADMM的现有收敛结果不适用。 在本论文中，我们将使用非凸和不精确加速的近端梯度（NIAPG）算法（姚等人，2017）。这是最近的非凸问题的近似算法。由于（16）中W和Z的正则化器是独立的，所以两个块的近端步长W.R.T.可以分别执行： 如上方所示，这些单独的近端步骤可以很容易地计算。 3.3.3.算法完善 COMPLETE ALGORITHM 整个过程将被称为“采样相关卷积稀疏编码（SCSC）”，在算法1中示出。它的空间复杂度为H(t) 和G（t），是O（R^2P）。其每次迭代时间复杂度为O（RKP+RP logP），其中O（RKP）项采用的是梯度计算，O（RP log P）是由于FFT/inverse FFT。表1比较了它的复杂性与其他在线和分布式CSC算法的复杂性。由此可见，SCSC具有较低的时间和空间复杂度。 4.实验 Experiments我们在多个数据集上进行实验（表2）。Fruit和City是在CSC文献中常用的两个小型的图像数据集。我们使用默认训练和测试分割。图像的预处理包括转换为灰度、特征标准化、局部对比度归一化和边缘渐变。这两个数据集很小，在后面的实验中，我们还将使用两个更大的数据集，CIFAR10（Krizhevsky &amp; Hinton, 2009）和Flower（Nilsback &amp; Zisserman, 2008）。随后我们将滤波器大小M设为11×11，以及正则化参数。 为了评估学习字典的有效性，我们将主要考虑图像重建的任务。重建的图像质量通过测试峰值信噪比（PAPYANN等人，2017）来评估： 其中X^j是测试集的X的重建。实验用不同的字典初始化重复五次。 4.1.W的选择：W1与W2 Choice of W : W`1 versus W`2首先，我们研究命题2中W的选择。我们比较SCSC-L1，它使用W= W’1，而SCSC-L2，使用的是W＝W’2。实验是在Fruit和City数据集上进行的。滤波器K的数目设置为100。回看表1中的空间复杂度的结果，我们定义SCSC相对于OCSC的压缩比（使用相同的K）作为CR＝（K/R）^2。我们在{K／20，K／10，K／9，…，K／2，K}中改变R的值，同时相应的CR为{400, 100, 81，.…，1 }。 结果如图1所示。可以看出，SCSC-L1是非常差的。图2（a）示出了在城市测试样本XJ上由K＝100和R＝10通过SCSC-L1获得的权重WJ（其他数据集上的结果是相似的）。可以看出，由于“1范数”引起的稀疏性，它的大部分条目都是零。表达能力严重受限，因为通常只使用一个基础滤波器来近似原始滤波器。另一方面，由SCSC-L2学习的WJ是稠密的并且具有更多的非零项（图2（b））。在续集中，我们只关注SCSC-L2，这将简单地表示为SCSC。 4.2. 样本自适应字典Sample-Dependent Dictionary在这个实验中，我们设置K＝100，并将SCSC与使用样本无关字典的下列算法进行比较：（i）SCSC（共享）：这是SCSC变体，其中所有Wi的In（5）都是相同的。其优化是基于交替最小化。（ii）通过张量分解（SET-TD）学习的可分离滤波器（SRONI等人，2015），这是基于后处理的（共享）字典，由OCSC学习的，如第2.1节所述；（iii）OCSC（王等人，2018）：最新的在线CSC算法。 结果如图3所示。可以看出，SCSC总是优于SCSC（共享）和SET-TD，当R＝10（对应于CR＝100）或以上时，SCSC优于OCSC。这表明使用依赖于样本的字典的优点。 接下来，我们与带精细调谐滤波器的OCSC进行比较，这也是样本自适应的。具体地，给定测试样本xj，我们首先从学习字典D获得（2）其代码Zj，然后通过使用新计算的Zj求解（3）来微调D。这是重复迭代的过程。我们设置OCSC的K等于SCSC的R，使这两种方法采取相同的空间复杂度（表1）。在SCSC中使用的K仍然是100。结果如图4所示。可以看出，虽然微调提高了OCSC的性能，但是这种产生样本自适应的滤波器的方法仍然比SCSC更差。 4.3.多滤波器学习 Learning with More Filters召回SCSC允许使用更多的过滤器（即较大的K），因为其较低的时间和空间复杂度。在这一节中，我们证明了这可以发挥更好的性能。我们比较SCSC与最近两批和在线CSC方法，即基于切片的CSC（SBCSC）（PaPaun等人，2017）和OCSC。对于SCSC，我们设定R＝10为Fruit和City，R＝30为CIFAR-10和Flower。 图5显示了在不同K的测试PSNR。如图所示，较大的K总是对所有方法都有更好的性能。SCSC允许使用更大的K，因为它的内存占用小得多。例如，在CIFAR-10上，CR＝1024时K＝800；在Flower上，CR＝1600时K＝400。 4.4.与目前最新技术的比较 Comparison with the State-of-the-Art首先，我们对Fruit和City的两个较小数据集进行实验，K = 100.我们为SCSC设置R = 10（即CR = 100）。 将其与批量CSC算法进行比较。 图6显示了与时钟时间的测试PSNR的收敛性。也证明了在线CSC算法收敛更快，相比于批量CSC方法具有更好的PSNR。在在线方法中，SCSC具有与OCSC类似的PSNR，但是收敛得更快，并且占用更少的存储（CR＝100）。 接下来，我们对两个大数据集CIFAR-10和Flower进行实验。所有的批处理CSC算法和两个在线CSC算法OCDL-Degraux和OCDL-Liu并不能处理这样大的数据集。因此，我们只比较SCSC与OCSC。在CIFAR-10上，我们设定K＝300，相应的SCSC的CR为100。在Flower上，SCSC的K值仍为300。然而，OCSC只能使用k＝50，因为它的内存占用太多多。图7显示了测试PSNR的收敛性。在这两种情况下，SCSC显著优于OCSC。 4.5.高维度数据 Higher-Dimensional Data在这一节中，我们对具有大于两个维度的数据集进行实验。为了缓解大的内存问题Choudhury等人提出了分布式算法的使用。在这里，我们表明，SCSC可以在一台机器上有效地处理这些数据集。 实验在三个数据集（表3）中进行（CoudHury等人，2017）。视频数据集包含在机场记录的图像子序列。每个视频的长度为7，每个图像帧的大小为100×100。多光谱数据包含60×60块来自多光谱图像（覆盖31个波长）的真实世界物体和材料。光场数据包含60×60块在物体和场景上的光场图像。对于每个像素，光线是来自8×8不同的方向。（将滤波器的大小M设为11×11×11视频，11×11×31为多光谱，11×11×8×8为光场。 我们比较SCSC与OCSC和共识 CSC（CCSC）算法，K＝50。为了公平的比较，所有的方法只使用一台机器。我们不与批处理方法和两种在线方法（OCDL-Degraux和OCDL-Liu）进行比较，因为它们是不可扩展的（如在第4.4节中已经示出的）。 由于SCSC的内存占用小，我们在GTX 1080 TI GPU上运行这个实验。OCSC也在GPU上运行用于视频。然而，OCSC只能在CPU上运行多光谱和光场。CCSC在处理过程中需要访问所有的样本和代码，也只能在CPU 上运行。 结果如表4所示。值得一提的是，SCSC是唯一可以处理整个视频、多光谱和光场数据集在一台机器上的算法。相比之下，CCSC只能处理最多30个视频样本、40个多光谱样本和35个光场样本。OCSC可以处理整个视频和多光谱，但是在使用整个光场数据集的2天内不能收敛。再一次表明SCSC优于OCSC和CCSC。 至于收敛速度，SCSC是最快的。但是注意这只是作为参考，因为SCSC是在GPU上运行的，而其他的（除了视频上的OCSC）在CPU上运行。然而，这仍然表明SCSC的一个重要优点，即它的小内存占用可以受益于GPU的使用，而其他的算法则不能。 4.6.图像去噪与修复 Image Denoising and Inpainting在以前的实验中，学习字典的优势是通过重建干净的图像来证明的。在这一节中，我们进一步研究学习字典在两个应用：图像去噪和修复。使用Choudhury等人提供的十个测试图像。在去噪中，我们加入零均值和方差为0.01的高斯噪声对图像进行测试（平均输入PSNR为10dB）。在修复中，我们将随机子样本的50%像素作为0（平均输入PSNR为912dB）。随后我们使用二进制权重矩阵来掩盖缺失像素的位置。我们使用在第4.4节中从水果中吸取的过滤器。将SCSC与（批）SBCSC和（在线）OCSC进行比较。结果如表5所示。可以看出，由SCSC获得的PSNR始终高于其他算法。这表明，在图像重建中产生高PSNR的字典在其他图像处理应用中也可以发挥更好的性能。 4.7. 求解（16）：niAPG与ADMM Solving (16): niAPG vs ADMM最后，我们比较了ADMM和niAPG在求解子问题（16）中的性能。我们使用来自City的训练样本xi。实验用不同的（Wi，Zi）初始化重复五次。图8显示了（16）目标随时间的收敛性。可见，niAPG具有快速收敛性，而ADMM不能收敛。图9显示了，它用迭代次数来衡量违反ADMM约束的情况。可以看出，违规不会达到零，这表明ADMM不收敛。 5.结论 Conclusion在本文中，我们提出了一种新的CSC扩展，其中每个样本都有自己的样本自适应的字典，由一组小的共享基滤波器构成。使用在线学习，该模型可以有效地更新，具有低的时间和空间复杂度。对包括大图像数据集和高维数据集在内的各种数据集的广泛实验都证明了它的效率和可扩展性。 才学疏浅，欢迎评论指导","categories":[{"name":"图像识别","slug":"图像识别","permalink":"https://wengjj.ink/categories/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/"},{"name":"论文翻译","slug":"图像识别/论文翻译","permalink":"https://wengjj.ink/categories/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/"}],"tags":[{"name":"样本自适应的在线卷积稀疏编码","slug":"样本自适应的在线卷积稀疏编码","permalink":"https://wengjj.ink/tags/%E6%A0%B7%E6%9C%AC%E8%87%AA%E9%80%82%E5%BA%94%E7%9A%84%E5%9C%A8%E7%BA%BF%E5%8D%B7%E7%A7%AF%E7%A8%80%E7%96%8F%E7%BC%96%E7%A0%81/"}]},{"title":"数据矿工学习-样本自适应的在线卷积稀疏编码论文简析","slug":"数据矿工学习-样本自适应的在线卷积稀疏编码论文简析","date":"2018-07-23T08:45:11.000Z","updated":"2020-08-30T08:54:42.344Z","comments":true,"path":"2018/07/23/数据矿工学习-样本自适应的在线卷积稀疏编码论文简析/","link":"","permalink":"https://wengjj.ink/2018/07/23/%E6%95%B0%E6%8D%AE%E7%9F%BF%E5%B7%A5%E5%AD%A6%E4%B9%A0-%E6%A0%B7%E6%9C%AC%E8%87%AA%E9%80%82%E5%BA%94%E7%9A%84%E5%9C%A8%E7%BA%BF%E5%8D%B7%E7%A7%AF%E7%A8%80%E7%96%8F%E7%BC%96%E7%A0%81%E8%AE%BA%E6%96%87%E7%AE%80%E6%9E%90/","excerpt":"在瑞典斯德哥尔摩国际会展中心举行的国际机器学习大会(ICML)正在受到全世界科技界的关注。来自国内人工智能企业队代表第四范式的姚权铭与来自香港科技大学的研究者提出的“Online Convolutional Sparse Coding with Sample-Dependent Dictionary：样本自适应的在线卷积稀疏编码”，入选了ICML 2018中选论文榜单。","text":"在瑞典斯德哥尔摩国际会展中心举行的国际机器学习大会(ICML)正在受到全世界科技界的关注。来自国内人工智能企业队代表第四范式的姚权铭与来自香港科技大学的研究者提出的“Online Convolutional Sparse Coding with Sample-Dependent Dictionary：样本自适应的在线卷积稀疏编码”，入选了ICML 2018中选论文榜单。 首先我们先通过思维导图来简要了解下这篇SCSC论文的整体结构： WHATSCSC是什么？ 卷积稀疏编码（CSC）已被广泛用于图像和信号处理中的平移不变字典(sample-dependent dictionary)的学习。不同于传统的CSC算法使用由所有样本共享的字典来卷积，此篇论文中的SCSC使用的是样本自适应的字典，其中每个过滤器是从数据中学习的一组基本滤波器的线性组合。这种增加的灵活性允许捕获大量依赖于样本的模式，这在处理大型或高维数据集时特别有用。在计算上，所得到的模型可以通过在线学习有效地学习。在大量的数据集上的实验结果表明，所提出的方法优于现有的CSC算法，具有显著减少的时间和空间复杂度。 WHYSCSC的优势在哪里？ 与目前的最新的CSC进行对比，SCSC的优势主要体现在三个方面: 1、数据集的大小 小样本数据集： 在小数据集的实验中，论文中将SCSC与批量CSC方法进行比较(包括DeconvNet、fast CSC、fast and flexible CSC等)，其中也包括着与SCSC一样采用在线方法的OCSC，实验的检验指标采用的是PSNR(峰值信噪比Peak Signal to Noise Ratio)，得到的结果如下图： 上方的图表现了各种CSC方法在时钟时间(clock time)下的PSNR收敛性，实验表明了小数据集条件下，在线CSC方法比批量CSC方法收敛的更快，具有更好的PSNR，而同样是在线方法的OCSC,SCSC虽然与OCSC具有类似的PSNR，但SCSC收敛的更快。 大样本数据集： 而在大数据集实验中，所有的批处理CSC算法和两个在线CSC算法OCDL DEGRAUX和OCDLLU不能处理这样大的数据集。因此，我们只比较SCSC与OCSC，比较的结果如下： 在CIFAR-10数据集上，我们设定SCSC和OCSC的K(滤波器数量)＝300。在Flower数据集上，SCSC的K值仍为300。然而，OCSC只能使用k＝50，因为它的内存占用大得多。图7显示了测试的PSNR的收敛性。在这两种情况下，SCSC显著优于OCSC。 2、高维度数据集下的表现 高维数组采用的是三种数据集：视频数据集、光谱数据集、光场数据集。研究人员将SCSC与OCSC和CousSUS CSC（CCSC）作对比，为了公平的比较，所有的方法只使用一台机器。值得一提的是，由于SCSC的内存占用小，实验人员可以在GTX 1080 TI GPU上运行这个实验。OCSC也在GPU上运行用于视频。然而，OCSC只能在CPU上运行多光谱和光场。CCSC在处理过程中需要访问所有的样本和代码，只能在CPU 上运行，实验的结果如下图： 根据论文中的实验结果显示，SCSC是唯一的可以处理整个视频，多光谱和光场数据集在一台机器上的方法。相比之下，CCSC只能处理最多30个视频样本、40个多光谱样本和35个光场样本。OCSC可以处理整个视频和多光谱，但是在使用整个光场数据集的2天内不能收敛。 至于速度，如Table 4所示SCSC的速度是最快的。但是值得注意的是，这仅仅只是作为参考，因为SCSC是在GPU上运行的，而其他的（除了视频数据集上的OCSC）都是在CPU上运行。然而，这仍然表明SCSC的一个重要优点，即它的小内存占用可以受益于GPU的使用，而其他的则不能。 3、图像的去噪与修复 在以前的实验中，学习字典的优势是通过重建干净的图像来证明的。此篇论文中研究人员进一步研究学习字典中的两个应用：图像去噪和修复。研究人员使用SCSC与（批处理CSC）SBCSC和（在线）OCSC进行比较。结果如表5所示: 可以看出，由SCSC获得的PSNR始终高于其他方法。这同时也表明，在图像重建中产生高PSNR的字典也在可以使其他图像处理应用发挥更好的性能。 HOWSCSC的使用细则 论文中提出的样本自适应的卷积稀疏编码(SCSC)主要解决传统卷积稀疏编码(CSC)不能适用于高维度数据(P表示)和较多过滤器(K表示)的问题；SCSC的核心有两点： (a)首先将CSC过滤器用两部分表示，第一部分是基础滤波器(base filters)所有样本共享，第二部分是样本自适应系数(sample-dependent weights)每个样本单独学习。这样一来，和标准CSC比较，SCSC方法中并没有全局的滤波器，而是对每个样本从一堆基础滤波器中通过样本自适应系数组合出来自己的滤波器。 (b)基于以上模型，只有基础滤波器是依赖于全部数据的。方法的第二点在于我们使用在线学习的方法去快速并且小内存的学习基础滤波器。 具体的SCSC学习算法如下： DICUSS关于SCSC的讨论与思考 随着CNN(卷积神经网络)在图像识别的效果越来越好，CNN越来越受AI学者的青睐，越来越多的应用也开始尝试采用CNN方法，但是，随着对CNN的尝试和研究的深入，它的不可解释性以及实验的不可复制重复的问题变的越来越严重，CNN是个黑盒魔法已几乎成为共识，相比与处理图像分类问题的CNN，CSC是一个线性卷积的无监督的学习的方法。CSC模型更简单，更直观容易分析理解。因此，最近一些机器学习&amp;机器视觉大牛(e.g. Michael Elad - 稀疏编码的创始人之一)开始尝试着用CSC解决应用问题和理解CNN，在应用层面上，医疗/生物图片数据，例如脑磁图(Magnetoencephalography, MEG)，(电子显微镜获得)，还有频谱(hyperspectral image)和光场(light field)数据上都使用CSC取得了非常不错的成果。 同时研究人员也表示，未来将结合自适应在线卷积稀疏编码SCSC和神经网络模型的优势，将样本自适应的idea应用到卷积神经网络模型中。这将增加神经网络迁移学习的能力同时减少其所需要的计算量，使得这些网络在高维度低样本数据上也适用。 论文链接：https://arxiv.org/pdf/1804.10366.pdf 才学疏浅，欢迎评论指导","categories":[{"name":"图像识别","slug":"图像识别","permalink":"https://wengjj.ink/categories/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/"},{"name":"论文简析","slug":"图像识别/论文简析","permalink":"https://wengjj.ink/categories/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/%E8%AE%BA%E6%96%87%E7%AE%80%E6%9E%90/"}],"tags":[{"name":"样本自适应的在线卷积稀疏编码","slug":"样本自适应的在线卷积稀疏编码","permalink":"https://wengjj.ink/tags/%E6%A0%B7%E6%9C%AC%E8%87%AA%E9%80%82%E5%BA%94%E7%9A%84%E5%9C%A8%E7%BA%BF%E5%8D%B7%E7%A7%AF%E7%A8%80%E7%96%8F%E7%BC%96%E7%A0%81/"}]},{"title":"数据矿工学习-《统计学习方法》思维导图7.1-线性可分支持向量机","slug":"数据矿工学习-《统计学习方法》思维导图7.1-线性可分支持向量机","date":"2018-07-18T02:16:27.000Z","updated":"2020-08-30T08:01:17.278Z","comments":true,"path":"2018/07/18/数据矿工学习-《统计学习方法》思维导图7.1-线性可分支持向量机/","link":"","permalink":"https://wengjj.ink/2018/07/18/%E6%95%B0%E6%8D%AE%E7%9F%BF%E5%B7%A5%E5%AD%A6%E4%B9%A0-%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE7.1-%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/","excerpt":"在机器学习的学习之路上，SVM是ML童鞋们在分类问题上一定会遇到的一个学习方法，SVM这一章将SVM按训练数据的线性可分性分为由简到繁分为三种模型：线性可分支持向量机(linner support vector machine in linearly separable case)、线性支持向量机(linear support vector machine)以及非线性支持向量机(non-linear support vector)。在实际工程中，面对不同规模的数据集时，在小型数据集上，SVM有时仅需小量的训练数据，就可以得到比较好的训练结果，而这正是因为SVM的特性-支持向量，下面通过思维导图先简单介绍下SVM：","text":"在机器学习的学习之路上，SVM是ML童鞋们在分类问题上一定会遇到的一个学习方法，SVM这一章将SVM按训练数据的线性可分性分为由简到繁分为三种模型：线性可分支持向量机(linner support vector machine in linearly separable case)、线性支持向量机(linear support vector machine)以及非线性支持向量机(non-linear support vector)。在实际工程中，面对不同规模的数据集时，在小型数据集上，SVM有时仅需小量的训练数据，就可以得到比较好的训练结果，而这正是因为SVM的特性-支持向量，下面通过思维导图先简单介绍下SVM： 线性可分支持向量机思维来自《统计学习方法》-李航 凹脑图在线浏览地址：线性可分支持向量机才学疏浅，欢迎评论指导","categories":[{"name":"《统计学习方法-李航》","slug":"《统计学习方法-李航》","permalink":"https://wengjj.ink/categories/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9D%8E%E8%88%AA%E3%80%8B/"}],"tags":[{"name":"线性可分支持向量机","slug":"线性可分支持向量机","permalink":"https://wengjj.ink/tags/%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"}]},{"title":"数据矿工学习-情感分析框架DeepEmo论文-个人中文翻译","slug":"数据矿工学习-情感分析框架DeepEmo论文-个人中文翻译","date":"2018-06-24T16:13:35.000Z","updated":"2020-08-30T08:59:59.048Z","comments":true,"path":"2018/06/25/数据矿工学习-情感分析框架DeepEmo论文-个人中文翻译/","link":"","permalink":"https://wengjj.ink/2018/06/25/%E6%95%B0%E6%8D%AE%E7%9F%BF%E5%B7%A5%E5%AD%A6%E4%B9%A0-%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E6%A1%86%E6%9E%B6DeepEmo%E8%AE%BA%E6%96%87-%E4%B8%AA%E4%BA%BA%E4%B8%AD%E6%96%87%E7%BF%BB%E8%AF%91/","excerpt":"作者： Elvis Saravia /National Tsing Hua University Hsinchu, Taiwan /ellfae@gmail.com Hsien-Chi Toby Liu /National Tsing Hua University Hsinchu, Taiwan /tobbymailbox@gmail.com Yi-Shin Chen /National Tsing Hua University Hsinchu, Taiwan /yishin@gmail.com DeepEmo论文地址： DeepEmo: Learning and Enriching Pattern-Based Emotion Representations","text":"作者： Elvis Saravia /National Tsing Hua University Hsinchu, Taiwan /ellfae@gmail.com Hsien-Chi Toby Liu /National Tsing Hua University Hsinchu, Taiwan /tobbymailbox@gmail.com Yi-Shin Chen /National Tsing Hua University Hsinchu, Taiwan /yishin@gmail.com DeepEmo论文地址： DeepEmo: Learning and Enriching Pattern-Based Emotion Representations 概述 Abstract我们提出了一个基于图论的算法来提取丰富的情感承载模式，从语料库中促进对线上情绪表达的深入分析。然后通过词语嵌入来丰富模式，并通过几种情绪识别任务进行评估。 此外，我们对情绪导向模式进行分析以证明其适用性并探索其性质。我们的实验结果表明，这次提出的技术胜过了大多数最新的情感识别技术。 1引言 Introduction情绪可以被定义为有意识的情感态度，这构成了一种感受的展示。情绪分类任务包括情感词汇和短语的表示学习或手动特征提取。虽然关于什么构成情感的问题一直存在争议，但情感识别模型及其应用可以提供的社会与经济的好处无疑是存在的。情绪是影响人类社会行为的关键因素，如动机，兴趣，讽刺和心理健康。最近，情感检测能力已经嵌入到感知意识，AI对话代理，如Woebot提出的对话系统中。我们工作的动机源于为更好地建模和探索不同形式的线上情绪表达的需求，特别是隐式表达。所提出的情绪表征允许情绪识别系统考虑停用词等语言组成部分，这些组成部分通常在情绪分析中被忽略，但却是我们如何表达我们的情绪和观点的一个重要组成部分。 来自文本的情感识别是具有挑战性的，因为情绪表达可以是高度隐含的并且随着时间的推移而变化。当依靠由人工制作的语言规则产生的资源（例如，情感词典）时，这提出了挑战。例如，当应用传统的特征提取器如词汇包和n-gram包时，错误发音的单词出现在一起将不会被识别为相同。在线社交网络中的另一个常见趋势是使用不同形式的表达方式，如俚语，代码字和表情符号来表达感受和意见。为了解决这个问题，我们设计了一个基于图论的算法，使提取情绪表示的过程自动化。 作一个概述，我们首先通过嘈杂的标签收集一个情感语料库，然后通过远程监督进行修改。然后，通过基于图论的机制提取情感特征，这些机制进一步丰富了词语嵌入，以保持模式之间的语义含义。为了评估模式的质量，使用各种在线分类器和深度学习模型来训练情感检测模型。本文的主要贡献如下：1）基于图论的自动情感特征提取机制，2）一组情感丰富的特征表示，用于组合各种情感识别任务和其他相关目标任务， 3）对各种传统学习模型和深度学习模型进行综合性能分析，因为它适用于从文本中进行情感识别; 4）情感丰富的词典，作为开源提供，允许对给定的情绪相关语料库进行更深入的分析。 2相关工作 Related Work2.1特征表示概述 Overview of Feature Representations我们将各种特征提取器与提出的技术在两个维度上进行比较：1）覆盖率 - 特征应该能够隐藏重要的隐性和显性情绪信息; 2）适应性 - 这些特征可以应用于其他类型的情绪语料库，起源于不同的领域。最近的情感识别系统采用表征学习进行特征检测。一般来说，词语嵌入（如word2vec）作为输入和深度学习模型（如卷积神经网络（CNN））的组合在句子分类中表现良好。由于这些类型的模型的性质和他们学习的特征类型，它们往往具有较高的覆盖率，较高的适应性，需要较少的监督（即自动学习特征），并且能在一定程度上捕捉上下文。然而，这种类型的模型在可解释性和高性能之间存在一定的权衡。我们基于图论的特征提取机制更侧重于语言组件之间的底层交互。因此，模式会自动显示隐式和明确的情绪表达。 2.2情感语料库和模型 Overview of Feature Representations目前有几个开放的情感数据集，如SemEval-2007情感文本任务（Strappa-rava和Mihalcea，2007）和奥运会数据集（Sintsova等，2013）。然而，这些情绪数据集要么由于缺乏细粒度的情感标签或数量而受到限制。我们引导了一组用于获取大量情感推文的嘈杂标签，然后通过远程监督执行注释。在情感识别研究中，普遍采用Plutchik的情感轮（Plutchik，2001）或Ekman的六种基本情绪（Ekman，1992）来定义情感类别。表情符号和表情符号也被证明可用于定义情绪类别。类似于（Moham-mad和Kiritchenko，2015; Liew和Turtle，2016; Abdul-Mageed和Ungar，2017），我们依靠标签来定义我们的情绪类别。 2.3情绪词典 Emotion Lexica情绪分类器使人们能够理解精神健康患者的心境模式。这些研究中的一些依赖于预先定义的词典，例如LIWC（Pennebaker et al。，2007）2，WordNet Affect（Strapparava et al。，2004）和EmoLex（Mohammad and Turney，2013），都是从基于文本的语料库中提取情感线索。最近的一项研究表明，社交网络中用户之间的情感态度和感知与人口特征之间存在相关性。这项研究依赖于一种情绪检测系统，该系统使用词汇特征（例如表情符号和主题标签）建立。其他用户信息（例如年龄和性别）则是从外部获得的，这限制了自己收集的数据量。对他们工作的改进是使用用户推文中的内容自动确定用户属性，如年龄和性别。其他情绪分类器使用手工制作的语言特征来提高情感分类性能。这些特征对于情感分类很有用，但存在有限的覆盖范围。我们的情绪词典是强调覆盖面（即隐含和明确的情感表达）。 3方法论 Methodology3.1基于图论的表示 Graph-Based Representations在本节中，我们介绍一种基于图论的特征提取算法，它可以自动提取一组情感丰富的句法模式。为了方便标识，我们用斜体（例如u）表示标量，粗体小写（例如v）表示向量和粗体大写（例如X）表示矩阵。模式P = {p1, p2, …, pn} 将分配到一个权重，也称为模式分数，用于确定模式p对情绪e的重要性。在情感分类的背景下，模式及其权重扮演着特征的角色。基于图的特征提取算法总结在以下步骤中： 步骤1（标准化）：首先，使用Twitter API 获取两个单独的文档集合 - 主观推文S（通过主题标签获得的嘈杂标签）和客观推文O（从新闻账户中获得）。这两个数据集都由空格进行标记，然后通过分别应用小写和使用和占位符替换用户提及和URL进一步进行预处理。 哈希标签用于获得文本中的基本事实，因此为了避免任何偏见，我们用替换它们。 步骤2（图构造）：给定标准化后的目标推文O和主观推文S，构造两个图：分别为客观图Go（Vo; Ao）和主观图Gs（Vs; As）。顶点V是一组表示从文集中提取的词条的节点。表示为A的边表示使用窗口方法从一段文本中提取的单词的关系。这个考虑是重要的，因为它保留了文本数据的韵律和潜在的句法结构。例如，“last night’s concert was just awesome !!!!! ”由此产生以下的一组弧：“ → last”, “last → night”, … , “!!!!! → ”。 步骤3（图聚合）：这一步的目标是获得一组与主观性或情绪表达更相关的弧。作为假设，通过用Go调整图形Gs，可以获得新的图形Ge，也被称为情感图形。 Ge保留与情绪相关的词条，分两步实现：（1）对于弧ai∈A，可以如公式1所示计算其归一化权重。freq（ai）是弧ai的频率 （2）随后，根据公式2中所示的一对调整来分配弧ai∈Ge的新权重。归因于图Ge的结果权重被调整，目标集合Go中最频繁出现的环弧在Ge中被削弱。 因此，具有较高权重的Ge中的弧可以表示与主观内容更相关的词条。 此外，修剪弧ai∈Ae，基于阈值φw。 步骤4（词条分类）：在给定邻接矩阵M的情况下，条目Mi,j被计算为：然后，计算Ve中所有顶点的特征向量中心性和聚类系数，将其用于将词条分为两类： 连接词和主题词。（1）连接词：为了测量图G中所有节点的影响，我们使用特征向量中心性，其计算公式如下：其中λ表示比例因子，ci是节点i的中心性分数。给定作为相应的特征值，方程4可以以向量表示形式重新表示为Mc = = λc，其中c是M的特征向量。给定选定的特征向量c和节点i的特征向量中心性分数，记为ci， 通过保留具有ci&gt; φeig的所有词条来获得连接词的最终列表（以下称为CW）。CW表示非常频繁且包含高中心性的词的集合（例如，“或”，“和” 和“我的”）。 （2）主题词：相反，主题词(subject words)或话题词(topical words)通常聚集在一起，即许多主题词通过相同的连接词相互连接。 因此，将一个系数分配给Ge中的所有节点，并计算如下：其中cli表示节点i的平均聚类系数，其捕获节点i的邻居之间的互连连接量。类似于连接词（以下称为CW），主题词（以下称为SW）通过重新获得所有具有cli&gt; φc1的词条来获得。 步骤5（模式候选者）：给定一组词条，SW和CW，我们采用bootstrap方法来构造候选模式，这些候选模式可以表达主观意义而且不会丢失句法结构。因此，以下是用于定义候选模式的一些规则：&lt;sw; sw; cw&gt;，&lt;sw; cw; sw&gt;，&lt;cw; sw; sw&gt;和&lt;cw; cw; sw&gt;，其中sw和cw分别表示从集合SW和CW中获得的任意词条。重要的是要澄清，在这项工作中使用了二号和三号的序列，因为这种设置对我们来说是最有效的。我们可能有时将这些候选模式称为模板。我们的工作的不同之处在于，我们不在模式提取过程中强加语法启发式或规则，因此，我们的模式倾向于自然而然地具有更高的覆盖率并能够捕捉隐含的情绪内容。 步骤6（基本模式提取）：朴素模式提取过程包括将句法模板以穷举的方式应用于训练语料库。另外，每个模式中的主题词sw被替换为一个占位符&lt;*&gt;。这个操作允许在我们的训练语料库中不存在的未知主语词，且对外部情绪模型建模时才被考虑。我们对与主观性高度相关的模式感兴趣，因此经常出现在阈值以上的模式被保留，其余模式被滤除。在表1中，我们提供了与相应模板一起提取的基本模式类型的实例。接下来，我们讨论用词嵌入方法来丰富句法模式的过程。这种丰富过程有助于保持模式之间的语义并提高特征相关性。 3.2富集模式 Enriched Patterns加权词嵌入：首先，我们获取来自（Deriu等，2017）和基于Twitter预先训练的的词嵌入，并通过远程监督通过情感语料库对其进行重新评估，我们通过反向传播训练了具有10个epoch（1个隐藏层）的完全连接的深度神经网络。我们将情感字嵌入为W ∈ R d×n ，其中d = 52。注意，术语频率逆文档频率（tf-idf）用于减少词汇的词汇量（从140K到20K字）。 词汇集群：然后，我们通过词嵌入信息使用聚类算法来生成与语义相关的词汇集群。为了确定集群的质量，我们与WordNet-Affect synsets进行了比较，并进行了同质性和完整性测试。我们使用Ward的方法（Ward Jr，1963）作为链接年龄标准，余弦距离作为距离度量。最后，我们获得了k = 1500个簇。我们使用scikit-learn实现来执行单词聚类（http：// scikit-learn.org）。 富集模式构建：词组的目的是用它们来指导富集模式的过程。换句话说，模式将保持一些语义关系，这对分类问题有用。请注意，除了单词嵌入式集成以外，此过程与简单模式提取类似。这需要一个引导过程，这需要一个引导过程，其中情感语料库被处理，并且以穷举的方式搜索候选模式。。任何满足模板的情感语料库中的任何单词序列都会被保留，其余的都被过滤掉。另外，模板的sw组件必须是在上面定义的单词群中找到的单词。此外，出现&lt;10的模式将被滤出，产生总共187,647个图案。在第6节中，我们更深入地分析模式并提供示例。 3.3情感模式加权 Emotion Pattern Weighing上一步提取的模式仍未映射到任何特定的情感类别。在训练分类模型之前，需要使用模式称量机制。 类似于其他流行的称重机制，如tf-idf()，权重决定了模式对每个情感的重要性。所提出的模式称重方案是tf-idf的一种修改，被称为模式频率 - 逆情感频率（pf- ief），并分两步定义。 首先，我们计算pf为：其中freq(p,e)表示p in e的频率，并且pfp,e表示与情绪e相关的文本集合中的模式p的对数缩放频率，然后我们计算ie f为：反情感频率iefp是所有情绪类别中模式p的相关性的量度。最后，我们获得一个模式评分为：其中psp, e是反映模式p对情感类别e的重要性的最终分数。 4模型 Models4.1 DeepEmo我们所提出的框架，被称为DeepEmo，将多层CNN体系结构与所提出的基于图论的特征的矩阵形式相结合。 输入X ∈ R n×m表示嵌入矩阵，其中条目X i,j表示情感j中富集模式i的模式分数。我们使用零填充策略来调整嵌入（Kim，2014） 输入被送入2个1d卷积层，滤波器的大小为3和16.这个过程的输出通过一个非线性激活函数（即ReLU（Nair and Hinton，2010））并产生一个特征映射矩阵。 然后将大小为3的1-max汇集层应用于每个特征地图map。 汇集的结果按顺序馈入尺寸为512和128的两个隐藏层，每个隐层都使用0：8的dropout进行正则化。 我们选择了128个批次，并使用Adam优化器对7个时期进行了训练。 softmax函数用于生成最终分类。 我们使用Keras来实现CNN架构。 4.2矢量模型 Vector Model作为基准，我们提出了一个朴素矢量模型（EVM），它演示了3.1节中提出的基本模式的基本可用性和适用性。 模式权重是使用3.3节提出的模式称量机制获得的。 形式上，给定n个模式和m个情绪，我们可以将整个情感模型表示为矩阵EM ∈ R n×m。 条目EMi,j表示情感j中的基本模式i的等级，这是基于模式评分psi,j。 请注意，具有较高ps值的模式具有较低的排名值，因为它们与该特定情感更相关。 假设我们想要获得其描述的情感的社交推文tw，我们首先计算其频率向量f ∈ R n，其中入口fi表示输入社交帖子d中的模式i的频率。 我们计算情感分数为：其中es ∈ R m和条目esj对应于推文tw的情感j的最终情感分数。 这些值中的最小值的索引被选择为针对tw检测到的最终情绪。 4.3模型比较 Comparison Models4.3.1传统模型 Traditional modelss我们将DeepEmo与常用于句子分类的各种传统方法（例如，字袋（BoW），字符级（char），n-gram，TF-IDF）进行比较。用于训练这些模型的分类器是随机梯度下降 ）由scikit-learn提供的分类器。 4.3.2深度学习模型 Deep Learning models深度学习架构支持从文本信息中自动学习功能。 我们观察到，在用于情感分类的深度学习模型的学习器中，它们因输入的选择而异：预先训练的词/字符嵌入和端到端学习的词/字符表示。 我们的工作不同之处在于我们使用丰富的基于图的表示作为输入，因此我们认为与这些方法进行比较也很重要。 我们与卷积神经网络（CNN），递归神经网络（RNN），双向门控循环神经网络（GRNN）和词嵌入（word2vec）进行比较。 5实验 Experiments5.1数据 Data我们遵循（Mohammad，2012; Wang等人，2012; Abdul-Mageed和Ungar，2017）并构建一组标签（基于Plutchik的情感轮（Plutchik，2001）），以从Twitter API收集英文推文。具体而言，我们使用Plutchik的八种基本情绪：愤怒，预感，厌恶，恐惧，喜悦，悲伤，惊喜和信任。主题标签用作嘈杂的标签，允许通过远程监督对数据进行注释。总共定义了339个主题标签。为确保推文质量，我们遵循（Abdul-Mageed和Ungar，2017年）提出的预处理步骤，并将推文的最后位置中的标签视为基本事实。我们将数据分成训练（90％）和测试（10％）。表3提供了数据的最终分布以及每种情绪的标签示例列表。在下面的章节中，我们将评估富集模式对几种情感识别任务的有效性。我们使用F1分数作为评估指标，由于情绪数据集的不平衡性质，这种评估指标通常用于情绪识别研究。 5.2实验结果 Experimental Results传统特征提取器：从传统特征提取器获得的结果在表2中给出。如表所示，对于字符级和字级特征提取器，TF-IDF模型通常比基于计数的基本特征产生更好的结果。这些发现与（Zhang等人，2015）的工作一致，其中传统方法如n-gram TF-IDF在各种句子分类任务中被发现与神经网络相当。 模式方法的结果：使用基本的基于图论的模式的EVM和CNN-patt的结果是大多数常规方法中最差的。使用富集模式的DeepEmo获得比CNN-patt和EVM以及所有其他传统方法更好的结果（F1分数为67％）。事实上，我们的方法在所有情况下都能获得最好的F1分数。我们还可以观察到，与基本模式模型（CNN-patt）相比，使用富集模式（DeepEmo）时，性能有显着提升（+ 15％）。总的来说，我们可以观察到丰富的基于图的特征对于训练情感识别模型是可行的。 与最新技术的比较：我们还与已发表的文献比较了结果，这些文献利用Ekman的六种基本情绪使用情感识别系统。为了公平比较，我们将我们的数据集从八种情绪再现为六种情绪：愤怒，厌恶，恐惧，喜悦，悲伤和表现。如表5所示，我们的情绪识别系统取得了比（Volkova和Bachrach，2016）例外的大多数方法更好的结果（F1得分为0.72％）。他们的情绪识别系统比我们的表现更好（F1分数为78％），因为他们使用明确的语言特征，例如表情符号和主题标签。我们的功能更容易受到噪音的影响，因为我们的目标是获得更高的覆盖范围，以捕捉更多隐含的情绪表达。如果我们打算使用情感词典对情感数据集进行深度分析，这个考虑就很重要。另外，它们的功能是特定于域的，这意味着一些重要的功能（例如表情图标和主题标签）可能不适用于其他情感数据集。传统方法是小规模数据的识别任务的强大候选人，到达几百万规模时，CNN模型可能才会做的更好。我们计划继续扩大我们的数据集并改进模式权重，这是改进结果的可行方法。 深度学习的结果：我们提供了与各种深度学习模型的比较，并评估了Ekman的六种基本情绪。这些建筑是从已发布的资源中采用的。我们将富集模式作为嵌入到双向GRNN中，并在深度学习模型中达到最佳结果（准确率为0.65％），如表4所示。结果表明，模式也可以应用于除CNN之外的其他深度学习模式，这为进一步的探索和实验留下了机会。 情感数据集：我们使用富集模式对其他现有的情感数据集进行了实验。与我们所知的（Felbo等，2017）相比，我们在SemEval-2007情感文本任务上获得了更好的结果（F1分数为0.48％） ，相对于这个数据集持有最新的结果（0.37％）。我们直接使用他们的基准数据集并修改我们的模型以支持可用的情感标签。在SemEval-2017任务4中，我们获得了53％的F1分数。这些结果提供了更多的证据表明我们富集模式适用于其他情绪相关的任务和数据集。 6富集模式分析 Analysis of Enriched Patterns在本节中，我们将探索从基于性别的数据集中提取的富集模式。 我们从Twitter收集用户提要，并根据他们的内容通过Sap等的性别预测器将用户分类为男性和女性类。 这产生了一个性别数据集，我们也自己手动验证。我们随机抽样2000男性和2000女性，然后从每个用户中随机抽取100个推文。 这总共产生了40万条推文，我们通过用 &lt;= 5个词条件过滤掉推文进一步减少了推文数量。 推文的最终数量是294,792，我们使用DeepEmo进行分类。 我们使用富集模式对性别数据应用模式频率分析。丢弃男性和女性共享的模式，并分析每个性别数据集的1000个最常出现的模式。 在表7中提供了由女性和男性表达的由&lt;cw; sw&gt;和&lt;sw; cw&gt;模板捕获的最常见情绪模式。{}中的单词代表模式富集过程捕获的主题词。 我们可以观察到，主题词代表了诸如“鄙视”，“大喊”和“最孤单”等情感丰富的词语。 另一方面，连接词提供了上下文，这有助于更好地理解富集模式。我们目前正在调查社交媒体上是否存在针对性别的情感模式或表情。然而，从这里提出的原始分析得出结论还为时过早。我们仍然可以观察到，提供情境有助于讲述情绪表达背后的故事。 另一个有趣的研究方向是直接将模式用于性别预测。分析的目标是探索富集模式，并展示它们如何用于对情绪语料库进行更深入的分析。 模式覆盖率：我们计算了几个情感数据集上富集模式的覆盖时间。如表6所示，性别数据中89.4％的推文至少包含一种富集模式。 我们的模式还显示了来自不同领域的数据集的高覆盖率，如SST-2（76％），SST-5（71％）（Socher等，2013）和PsychExp（95％）（Wallbott和Scherer，1988）。我们观察到数据集大小不影响覆盖率结果。根据（Wallbott和Scherer，1988）描述的情绪体验获得了高覆盖率（95％），这些情绪体验来源于模式构建的不同领域。 这表明我们富集模式适用于其他领域，这为进一步的探索和实验提供了机会。 7讨论 DiscussionAbdul-Mageed和Ungar（2017）的研究表明，提高数据质量是改善情绪分类结果（达到83％的F1分数）的重要一步。我们观察到他们报告了一个更大的数据集（790,059）和更平衡的数据收集每个情感。相反，我们的数据集更不平衡，但即使平衡结果没有显着改善（平均F1分数为68％）。在撰写本文时，作者仍在努力公开提供他们的数据集，因此我们无法直接比较他们的方法。 作为未来的工作，我们希望不断完善我们的主题标签并改善情绪问题。 所有基准数据集，词典，预先训练的模型以及运行模型的代码都将很快推出。 8结论 Conclusion我们提出了一个丰富的基于图论的特征提取机制来提取丰富情感的表示。这些模式充满了词语嵌入，并用于训练多种有效的情感识别模型。我们的模式捕捉隐含的情绪表达，这些表情能够证明情绪识别结果，并有助于解释性。我们展示了拟建的情感词典对性别数据集的基本应用。我们希望改进模式称量机制，以提高情绪识别任务的性能，并尽量减少模式覆盖率和追求性能之间的平衡。 *DeepEmo论文地址： DeepEmo: Learning and Enriching Pattern-Based Emotion Representations * 才学疏浅，欢迎评论指导","categories":[{"name":"NLP","slug":"NLP","permalink":"https://wengjj.ink/categories/NLP/"},{"name":"论文翻译","slug":"NLP/论文翻译","permalink":"https://wengjj.ink/categories/NLP/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/"}],"tags":[{"name":"情感分析框架DeepEmo","slug":"情感分析框架DeepEmo","permalink":"https://wengjj.ink/tags/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E6%A1%86%E6%9E%B6DeepEmo/"}]},{"title":"数据矿工学习-情感分析框架DeepEmo论文简析","slug":"数据矿工学习-情感分析框架DeepEmo论文简析","date":"2018-06-20T09:08:44.000Z","updated":"2020-08-30T08:47:11.786Z","comments":true,"path":"2018/06/20/数据矿工学习-情感分析框架DeepEmo论文简析/","link":"","permalink":"https://wengjj.ink/2018/06/20/%E6%95%B0%E6%8D%AE%E7%9F%BF%E5%B7%A5%E5%AD%A6%E4%B9%A0-%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E6%A1%86%E6%9E%B6DeepEmo%E8%AE%BA%E6%96%87%E7%AE%80%E6%9E%90/","excerpt":"来自台湾国立清华大学的Elvis Saravia等研究人员提出了一种基于图论(graph-based)的机制来提取丰富情感(rich-emotion)的相关模式(pattern)，用来加强对语料库的线上情感表达进行深入分析。论文实验结果表明，所提出的情感分析框架DeepEmo比目前大多数的情感分析框架的F1-score都要高（仅次于Volvoka(2016)），而且其提出的富集模式(enriched patterns)也被证实了具有很高的领域适用性。","text":"来自台湾国立清华大学的Elvis Saravia等研究人员提出了一种基于图论(graph-based)的机制来提取丰富情感(rich-emotion)的相关模式(pattern)，用来加强对语料库的线上情感表达进行深入分析。论文实验结果表明，所提出的情感分析框架DeepEmo比目前大多数的情感分析框架的F1-score都要高（仅次于Volvoka(2016)），而且其提出的富集模式(enriched patterns)也被证实了具有很高的领域适用性。 首先我们先通过思维导图来简要了解下这篇DeepEmo论文的整体结构： WHAT情感分析框架DeepEmo是什么？DeepEmo论文提出了三种方法论，分别是： (1) 基于图论的基本模式提取方法；(2) 利用词嵌入的基本模式丰富方法-富集模式(enriched patterns)；(3) 对tf-idf(词频-逆文件频率)进行修改的pf- ief(模式频率 - 逆情感频率)评分机制； DeepEmo情感分析框架正是由这三种方法论搭配卷积神经网络CNN组成，由这三种方法论所生成的基于图论的嵌入矩阵，作为输入数据，最后由卷积神经网络CNN输出其情感类别。 WHY情感分析框架DeepEmo的优势在哪里？近些年的情感识别系统一般采用表征学习进行特征检测。一般来说，词语嵌入（如word2vec）作为输入和深度学习模型（如卷积神经网络（CNN））的组合在句子分类中表现良好。由于这些类型的模型的性质和他们学习的特征类型，它们往往具有较高的覆盖率(覆盖率 - 特征应该能够隐藏重要的隐性和显性情绪信息)，较高的适应性(适应性 - 这些特征可以应用于其他类型的情绪语料库，起源于不同的领域)，仅需要很少的监督（即自动学习特征），并且能在一定程度上捕捉上下文。然而，这种类型的模型在可解释性和高性能之间是存在一定的权衡的。基于图论的特征提取机制DeepEmo更侧重于语言组件之间的底层交互。因此，模式会自动显示隐式和明确的情绪表达。 为了验证DeepEmo的性能，研究人员将DeepEmo与scikit-learn提供的常用于句子分类的传统情感分析器（例如，字袋（BoW），字符级（char），n-gram，TF-IDF）进行比较，结果如下表：从这张表我们能看到一些较“反常”的情况，根据一些以往的经验来说，深度学习的效果应该比传统的学习方法效果要好，但从这张结果表中，我们可以看出基于TF-IDF的BoW和基于词频的BoW的得分都比基于基本模式的CNN模型识别效果都要好，从这里我们看出，深度学习器并不一定总比传统学习器的分类效果要好，从这里我们可以得到一个更为重要的论点，DeepEmo的情感识别效果好并不是因为使用了CNN，而是因为使用了富集模式(Enriched Patterns)。 HowDeepEmo使用细则目前的话，DeepEmo的研究人员还未将DeepEmo的模型代码放出来，所以现阶段我们要使用DeepEmo的话只能照着论文自己手打出来，这里就简要说明下DeepEmo的运行流程，让大家对DeepEmo的运行有个较为清晰的整体理解：深度学习架构支持从文本信息中自动学习的功能。目前在用于情感分类的深度学习模型的学习器中，它们因输入的选择而异：预先训练的词/字符嵌入和端到端学习的词/字符表示。DeepEmo的不同之处在于使用丰富的基于图论的表征矩阵作为输入，所以构建DeepEmo框架最重要的部分就是生成基于图论的矩阵，关于具体的生成，论文里面写的非常详细，有兴趣的同学可以自行查看(DeepEmo论文的中文版笔者我将在修改润色后发布)，在提取基本模式这一步上，由于DeepEmo的研究者并不在模式提取过程中强行添加语法启发或规则，因此，提取的基本模式倾向于自然而然地具有更高的覆盖率并更全面地捕捉隐含和明确的情感内容。 DISCUSS关于DeepEmo的讨论与思考 从DeepEmo研究人员的样本分布表中不难看出情感样本其实是具有一定的样本不均匀的，厌恶(disgust)和期盼(anticipation)的样本相对较少，但其实DeepEmo研究人员也发现了这个问题，但平衡样本后F1的平均分数并没有得到提升。 在与目前最新的论文所提出情感分类器相对比，DeepEmo情感识别系统取得了比除（Volkova和Bachrach，2016）例外的大多数方法有更好的结果（F1得分为72％）。Volvoka情绪识别系统比DeepEmo的表现更好（F1分数为78％），因为Volvoka使用更为明确的语言特征，例如表情符号和主题标签。而DeepEmo的数据集更容易受到噪音的影响，因为DeepEmo的目的是获得更高的覆盖范围，以捕捉更多隐含的情绪表达。但太过于明确的数据集也意味着生成的情感分析器可能不适用于其他的情感数据集，也就是泛化程度可能过低。 那目前DeepEmo的问题在哪里呢？在笔者看来，现在DeepEmo最大的问题在于数据集的规模太小，这个问题在吴恩达的深度学习课程第一周就有提到过，根据上方的图得知，深度学习的高性能一般得在百万级别的数据集的学习上才体现的出来，小样本学习深度学习器跟传统学习器效果差不多，所以目前DeepEmo要想有更上一步的性能提高，笔者认为可以从数据集的规模方面入手，给与DeepEmo更多的丰富样本去学习。 DeepEmo论文地址： DeepEmo: Learning and Enriching Pattern-Based Emotion Representations 才学疏浅，欢迎评论指导","categories":[{"name":"NLP","slug":"NLP","permalink":"https://wengjj.ink/categories/NLP/"},{"name":"论文简析","slug":"NLP/论文简析","permalink":"https://wengjj.ink/categories/NLP/%E8%AE%BA%E6%96%87%E7%AE%80%E6%9E%90/"}],"tags":[{"name":"情感分析框架DeepEmo","slug":"情感分析框架DeepEmo","permalink":"https://wengjj.ink/tags/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E6%A1%86%E6%9E%B6DeepEmo/"}]},{"title":"数据矿工学习-《统计学习方法》思维导图6.2-最大熵模型与最优化算法","slug":"数据矿工学习-《统计学习方法》思维导图6.2-最大熵模型与最优化算法","date":"2018-06-06T09:12:13.000Z","updated":"2020-08-30T07:59:27.129Z","comments":true,"path":"2018/06/06/数据矿工学习-《统计学习方法》思维导图6.2-最大熵模型与最优化算法/","link":"","permalink":"https://wengjj.ink/2018/06/06/%E6%95%B0%E6%8D%AE%E7%9F%BF%E5%B7%A5%E5%AD%A6%E4%B9%A0-%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE6.2-%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B%E4%B8%8E%E6%9C%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/","excerpt":"最大熵模型目前应用在NLP上比较多，本章的最大熵模型的内容与数学推导公式比较多（最优化算法部分本人水平有限(ˉ▽ˉ；)…，当时看的最优化算法时有点懵逼，可能会有疏漏，望各位海涵，欢迎提出修改意见）","text":"最大熵模型目前应用在NLP上比较多，本章的最大熵模型的内容与数学推导公式比较多（最优化算法部分本人水平有限(ˉ▽ˉ；)…，当时看的最优化算法时有点懵逼，可能会有疏漏，望各位海涵，欢迎提出修改意见） 思维来自《统计学习方法》-李航 凹脑图在线浏览地址：最大熵模型与最优化算法才学疏浅，欢迎评论指导","categories":[{"name":"《统计学习方法-李航》","slug":"《统计学习方法-李航》","permalink":"https://wengjj.ink/categories/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9D%8E%E8%88%AA%E3%80%8B/"}],"tags":[{"name":"最大熵模型","slug":"最大熵模型","permalink":"https://wengjj.ink/tags/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/"},{"name":"最优化算法","slug":"最优化算法","permalink":"https://wengjj.ink/tags/%E6%9C%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"}]},{"title":"数据矿工学习-《统计学习方法》思维导图6.1-逻辑斯蒂回归模型","slug":"数据矿工学习-《统计学习方法》思维导图6.1-逻辑斯蒂回归模型","date":"2018-06-06T09:10:13.000Z","updated":"2020-08-30T07:59:14.902Z","comments":true,"path":"2018/06/06/数据矿工学习-《统计学习方法》思维导图6.1-逻辑斯蒂回归模型/","link":"","permalink":"https://wengjj.ink/2018/06/06/%E6%95%B0%E6%8D%AE%E7%9F%BF%E5%B7%A5%E5%AD%A6%E4%B9%A0-%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE6.1-%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/","excerpt":"逻辑斯蒂回归模型是经典的分类学习器，在二分类的监督问题上分类效果非常好，其经典之处就在于LR的分布函数-sigmoid函数。","text":"逻辑斯蒂回归模型是经典的分类学习器，在二分类的监督问题上分类效果非常好，其经典之处就在于LR的分布函数-sigmoid函数。 思维来自《统计学习方法》-李航 凹脑图在线浏览地址：逻辑斯蒂回归模型 **才学疏浅，欢迎评论指导**","categories":[{"name":"《统计学习方法-李航》","slug":"《统计学习方法-李航》","permalink":"https://wengjj.ink/categories/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9D%8E%E8%88%AA%E3%80%8B/"}],"tags":[{"name":"逻辑斯蒂回归模型","slug":"逻辑斯蒂回归模型","permalink":"https://wengjj.ink/tags/%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/"}]},{"title":"数据矿工学习-Python数据分析之pandas","slug":"数据矿工学习-Python数据分析之pandas","date":"2018-05-21T11:26:12.000Z","updated":"2020-08-30T07:59:52.652Z","comments":true,"path":"2018/05/21/数据矿工学习-Python数据分析之pandas/","link":"","permalink":"https://wengjj.ink/2018/05/21/%E6%95%B0%E6%8D%AE%E7%9F%BF%E5%B7%A5%E5%AD%A6%E4%B9%A0-Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8Bpandas/","excerpt":"前言前面一篇文章我们介绍了numpy，但numpy的特长并不是在于数据处理，而是在它能非常方便地实现科学计算，所以我们日常对数据进行处理时用的numpy情况并不是很多，我们需要处理的数据一般都是带有列标签和index索引的，而numpy并不支持这些，这时我们就需要pandas上场啦！","text":"前言前面一篇文章我们介绍了numpy，但numpy的特长并不是在于数据处理，而是在它能非常方便地实现科学计算，所以我们日常对数据进行处理时用的numpy情况并不是很多，我们需要处理的数据一般都是带有列标签和index索引的，而numpy并不支持这些，这时我们就需要pandas上场啦！ PandasWHAT?Pandas是基于Numpy构建的库，在数据处理方面可以把它理解为numpy加强版，同时Pandas也是一项开源项目：Github 。不同于numpy的是，pandas拥有种数据结构：Series和DataFrame： 下面我们就来生成一个简单的series对象来方便理解： 12345678910In [1]: from pandas import Series,DataFrameIn [2]: import pandas as pdIn [3]: data = Series([1,2,3,4],index = ['a','b','c','d'])In [4]: dataOut[4]:a 1b 2c 3d 4dtype: int64 Series是一种类似一维数组的数据结构，由一组数据和与之相关的index组成，这个结构一看似乎与dict字典差不多，我们知道字典是一种无序的数据结构，而pandas中的Series的数据结构不一样，它相当于定长有序的字典，并且它的index和value之间是独立的，两者的索引还是有区别的，Series的index是可变的，而dict字典的key值是不可变的。 下面照例生成一个简单的DataFrame对象： 12345678In [8]: data = &#123;'a':[1,2,3],'b':['we','you','they'],'c':['btc','eos','ae']&#125;In [9]: df = DataFrame(data)In [10]: dfOut[10]: a b c0 1 we btc1 2 you eos2 3 they ae DataFrame这种数据结构我们可以把它看作是一张二维表，DataFrame长得跟我们平时使用的Excel表格差不多，DataFrame的横行称为columns，竖列和Series一样称为index，DataFrame每一列可以是不同类型的值集合，所以DataFrame你也可以把它视为不同数据类型同一index的Series集合。 WHY?科学计算方面numpy是优势，但在数据处理方面DataFrame就更胜一筹了，事实上DataFrame已经覆盖了一部分的数据操作了，对于数据挖掘来说，工作可大概分为读取数据-数据清洗-分析建模-结果展示： 先说说读取数据，Pandas提供强大的IO读取工具，csv格式、Excel文件、数据库等都可以非常简便地读取，对于大数据，pandas也支持大文件的分块读取； 接下来就是数据清洗，面对数据集，我们遇到最多的情况就是存在缺失值，Pandas把各种类型数据类型的缺失值统一称为NaN（这里要多说几句，None==None这个结果是true，但np.nan==np.nan这个结果是false，NaN在官方文档中定义的是float类型，有关于NaN和None的区别以及使用，有位博主已经做好整理：None vs NaN）,Pandas提供许多方便快捷的方法来处理这些缺失值NaN。 最重要的分析建模阶段，Pandas自动且明确的数据对齐特性，非常方便地使新的对象可以正确地与一组标签对齐，有了这个特性，Pandas就可以非常方便地将数据集进行拆分-重组操作。 最后就是结果展示阶段了，我们都知道Matplotlib是个数据视图化的好工具，Pandas与Matplotlib搭配，不用复杂的代码，就可以生成多种多样的数据视图。 HOW？SeriesSeries的两种生成方式： 12345678In [19]: data = Series([222,'btc',234,'eos'])In [20]: dataOut[20]:0 2221 btc2 2343 eosdtype: object 虽然我们在生成的时候没有设置index值，但Series还是会自动帮我们生成index，这种方式生成的Series结构跟list列表差不多，可以把这种形式的Series理解为竖起来的list列表。 12345678In [21]: data = Series([1,2,3,4],index = ['a','b','c','d'])In [22]: dataOut[22]:a 1b 2c 3d 4dtype: int64 这种形式的Series可以理解为numpy的array外面披了一件index的马甲，所以array的相关操作，Series同样也是支持的。结构非常相似的dict字典同样也是可以转化为Series格式的： 12In [29]: dic = &#123;'a':1,'b':2,'c':'as'&#125; In [30]: dicSeries = Series(dic) 查看Series的相关信息： 123456In [32]: data.index Out[32]: Index(['a', 'b', 'c', 'd'], dtype='object') In [33]: data.values Out[33]: array([1, 2, 3, 4], dtype=int64) In [35]: 'a' in data #in方法默认判断的是index值 Out[35]: True Series的NaN生成： 12345678910In [46]: index1 = [ 'a','b','c','d']In [47]: dic = &#123;'b':1,'c':1,'d':1&#125;In [48]: data2 = Series(dic,index=index1)In [49]: data2Out[49]:a NaNb 1.0c 1.0d 1.0dtype: float64 从这里我们可以看出Series的生成依据的是index值，index‘a’在字典dic的key中并不存在，Series自然也找不到’a’的对应value值，这种情况下Pandas就会自动生成NaN(not a number)来填补缺失值，这里还有个有趣的现象，原本dtype是int类型，生成NaN后就变成了float类型了，因为NaN的官方定义就是float类型。 NaN的相关查询： 1234567891011121314151617181920212223In [58]: data2.isnull()Out[58]:a Trueb Falsec Falsed Falsedtype: bool In [59]: data2.notnull()Out[59]:a Falseb Truec Trued Truedtype: bool In [60]: data2[data2.isnull()==True] #嵌套查询NaNOut[60]:a NaNdtype: float64 In [64]: data2.count() #统计非NaN个数Out[64]: 3 切记切记，查询NaN值切记不要使用np.nan==np.nan这种形式来作为判断条件，结果永远是False，==是用作值判断的，而NaN并没有值，如果你不想使用上方的判断方法，你可以使用is作为判断方法，is是对象引用判断，np.nan is np.nan，结果就是你要的True。 Series自动对齐： 1234567891011121314151617181920212223In [72]: data1Out[72]:a 1asd 1b 1dtype: int64 In [73]: dataOut[73]:a 1b 2c 3d 4dtype: int64 In [74]: data+data1Out[74]:a 2.0asd NaNb 3.0c NaNd NaNdtype: float64 从上面两个Series中不难看出各自的index所处位置并不完全相同，这时Series的自动对齐特性就发挥作用了，在算术运算中，Series会自动寻找匹配的index值进行运算，如果index不存在匹配则自动赋予NaN,值得注意的是，任何数+NaN=NaN,你可以把NaN理解为吸收一切的黑洞。 Series的name属性： 12345678910In [84]: data.index.name = 'abc'In [85]: data.name = 'test'In [86]: dataOut[86]:abca 1b 2c 3d 4Name: test, dtype: int64 Series对象本身及其索引index都有一个name属性，name属性主要发挥作用是在DataFrame中，当我们把一个Series对象放进DataFrame中，新的列将根据我们的name属性对该列进行命名，如果我们没有给Series命名，DataFrame则会自动帮我们命名为0。 DataFrameDataFrame的生成： 12345678In [87]: data = &#123;'name': ['BTC', 'ETH', 'EOS'], 'price':[50000, 4000, 150]&#125;In [88]: data = DataFrame(data)In [89]: dataOut[89]: name price0 BTC 500001 ETH 40002 EOS 150 DataFrame的生成与Series差不多，你可以自己指定index，也可不指定，DataFrame会自动帮你补上。 查看DataFrame的相关信息： 1234567891011In [95]: data.indexOut[95]: RangeIndex(start=0, stop=3, step=1) In [96]: data.valuesOut[96]:array([['BTC', 50000], ['ETH', 4000], ['EOS', 150]], dtype=object) In [97]: data.columns #DataFrame的列标签Out[97]: Index(['name', 'price'], dtype='object') DataFrame的索引： 12345678910111213141516171819In [92]: data.nameOut[92]:0 BTC1 ETH2 EOSName: name, dtype: object In [93]: data['name']Out[93]:0 BTC1 ETH2 EOSName: name, dtype: object In [94]: data.iloc[1] #loc['name']查询的是行标签Out[94]:name ETHprice 4000Name: 1, dtype: object 其实行索引，除了iloc，loc还有个ix，ix既可以进行行标签索引，也可以进行行号索引，但这也大大增加了它的不确定性，有时会出现一些奇怪的问题，所以pandas在0.20.0版本的时候就把ix给弃用了。 DataFrame的常用操作： 简单地增加行、列： 12345678In [105]: data['type'] = 'token' #增加列 In [106]: dataOut[106]: name price type0 BTC 50000 token1 ETH 4000 token2 EOS 150 token 123456789In [109]: data.loc['3'] = ['ae',200,'token'] #增加行 In [110]: dataOut[110]: name price type0 BTC 50000 token1 ETH 4000 token2 EOS 150 token3 ae 200 token 删除行、列操作： 123456789In [117]: del data['type'] #删除列 In [118]: dataOut[118]: name price0 BTC 500001 ETH 40002 EOS 1503 ae 200 1234567891011121314In [120]: data.drop([2]) #删除行Out[120]: name price0 BTC 500001 ETH 40003 ae 200 In [121]: dataOut[121]: name price0 BTC 500001 ETH 40002 EOS 1503 ae 200 这里需要注意的是，使用drop（）方法返回的是Copy而不是视图，要想真正在原数据里删除行，就要设置inplace=True： 12345678In [125]: data.drop([2],inplace=True) In [126]: dataOut[126]: name price0 BTC 500001 ETH 40003 ae 200 设置某一列为index： 123456789101112131415161718In [131]: data.set_index(['name'],inplace=True) In [132]: dataOut[132]: pricenameBTC 50000ETH 4000ae 200 In [133]: data.reset_index(inplace=True) #将index返回回dataframe中 In [134]: dataOut[134]: name price0 BTC 500001 ETH 40002 ae 200 处理缺失值： 12345678910111213141516171819202122In [149]: dataOut[149]: name price0 BTC 50000.01 ETH 4000.02 ae 200.03 eos NaN In [150]: data.dropna() #丢弃含有缺失值的行Out[150]: name price0 BTC 50000.01 ETH 4000.02 ae 200.0 In [151]: data.fillna(0) #填充缺失值数据为0Out[151]: name price0 BTC 50000.01 ETH 4000.02 ae 200.03 eos 0.0 还是需要注意：这些方法返回的是copy而不是视图，如果想在原数据上改变，别忘了inplace=True。 数据合并： 1234567891011121314151617181920212223In [160]: dataOut[160]: name price0 BTC 50000.01 ETH 4000.02 ae 200.03 eos NaN In [161]: data1Out[161]: name other0 BTC 500001 BTC 40002 EOS 150 In [162]: pd.merge(data,data1,on='name',how='left') #以name为key进行左连接Out[162]: name price other0 BTC 50000.0 50000.01 BTC 50000.0 4000.02 ETH 4000.0 NaN3 ae 200.0 NaN4 eos NaN NaN 平时进行数据合并操作，更多的会出一种情况，那就是出现重复值，DataFrame也为我们提供了简便的方法： 1data.drop_duplicates(inplace=True) 数据的简单保存与读取： 123456789In [165]: data.to_csv('test.csv') In [166]: pd.read_csv('test.csv')Out[166]: Unnamed: 0 name price0 0 BTC 50000.01 1 ETH 4000.02 2 ae 200.03 3 eos NaN 为什么会出现这种情况呢，从头看到尾的同学可能就看出来了，增加第三行时，我用的是loc[‘3’]行标签来增加的，而read_csv方法是默认index是从0开始增长的，此时只需要我们设置下index参数就ok了： 12345678In [167]: data.to_csv('test.csv',index=None) #不保存行索引In [168]: pd.read_csv('test.csv')Out[168]: name price0 BTC 50000.01 ETH 4000.02 ae 200.03 eos NaN 其他的还有header参数，这些参数都是我们在保存数据时需要注意的。 总结：pandas的操作和方法肯定远远不止于这些，查看这些方法最好的方法就是查看官方文档，pandas还有许多强大有用的方法，例如大数据读取，时间序列等等，这些就靠大家自己去pandas官方文档好好挖掘了。 参考资料：Liu Lixiang的博客 YXiao’s Blog极客学院-《从零开始学Python-第二版》Pandas 0.23.0官方文档 才学疏浅，欢迎评论指导","categories":[{"name":"Python","slug":"Python","permalink":"https://wengjj.ink/categories/Python/"}],"tags":[{"name":"pandas","slug":"pandas","permalink":"https://wengjj.ink/tags/pandas/"}]},{"title":"数据矿工学习-Python数据分析之numpy","slug":"数据矿工学习-Python数据分析之numpy","date":"2018-05-21T09:37:20.000Z","updated":"2020-08-30T08:01:31.319Z","comments":true,"path":"2018/05/21/数据矿工学习-Python数据分析之numpy/","link":"","permalink":"https://wengjj.ink/2018/05/21/%E6%95%B0%E6%8D%AE%E7%9F%BF%E5%B7%A5%E5%AD%A6%E4%B9%A0-Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8Bnumpy/","excerpt":"前言刚开始学习数据挖掘时，有时总是对numpy和pandas傻傻分不清楚，这个问题在训练模型阶段输入训练数据的时候最为明显，下面就来详细介绍下numpy","text":"前言刚开始学习数据挖掘时，有时总是对numpy和pandas傻傻分不清楚，这个问题在训练模型阶段输入训练数据的时候最为明显，下面就来详细介绍下numpy NumpyWHAT？numpy是专门为科学计算设计的一个python扩展包，为python提供高效率的多维数组，也被称为面向阵列计算（array oriented computing），同时numpy也是github上的一个开源项目：numpy，numpy是基于c语言开发，所以这使得numpy的运行速度很快，高效率运行就是numpy的一大优势。 下面我们就来生成一个最简单的numpy对象，首先我们要导入numpy包，一般我们都把它命名为np： 1In [1]: import numpy as np 接着就可以生成一个numpy一维数组： 123In [2]: a = np.array([[1,2,3]],dtype=np.int32) In [3]: a Out[3]: array([1, 2, 3]) numpy中定义的最重要的数据结构是称为ndarray的n维数组类型，这个结构引用了两个对象，一块用于保存数据的存储区域和一个用于描述元素类型的dtype对象： WHY？二维数组的生成在python中我们还可以用到list列表，如果用list来表示[1,2,3]，由于list中的元素可以是任何对象，所以list中保存的是对象的指针，如果要保存[1,2,3]就需要三个指针和三个整数对象，是比较浪费内存资源和cpu计算时间的，而ndarray是一种保存单一数据类型的多维数组结构，在数据处理上比list列表要快上很多，在这里我们可以用%timeit命令来检测两者的数据处理速度： 123456In [9]: a = range(1000) In [10]: %timeit[i**2 for i in a] 381 µs ± 6.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) In [11]: b = np.arange(1000) In [12]: %timeit b**2 1.41 µs ± 18 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each) 由于相同数据大小的array运算直接作用到元素级上这一numpy特性，结果显而易见，在数据处理上numpy数组比使用for循环的list列表快的不是一点两点。 HOW?OK,知道了numpy大概是个什么东西，下面我们就来介绍下numpy数组的一些常用操作： 这里生成一个3x3的矩阵作为例子： 123456In [2]: data = np.array([[1,2,3],[4,5,6],[7,8,9]]) #等价于data=np.arange(1，10).reshape(3,3)In [3]: dataOut[3]:array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) 查看矩阵信息： 123456In [6]: data.shape #返回元组，表示n行n列 Out[6]: (3, 3) In [7]: data.dtype #返回数组数据类型Out[7]: dtype('int32') In [8]: data.ndim #返回是几维数组 Out[8]: 2 转换数据类型： 123In [11]: a = data.astype(float) #拷贝一份新的数组 In [12]: a.dtype Out[12]: dtype('float64') 数组之间的计算： 1234567891011In [15]: data+dataOut[15]:array([[ 2, 4, 6], [ 8, 10, 12], [14, 16, 18]]) In [16]: data*dataOut[16]:array([[ 1, 4, 9], [16, 25, 36], [49, 64, 81]]) 可以看出相同规格的数组计算是直接作用在其元素级上的，那不同的规格的数组是否能进行运算呢，我们来看下这个例子： 123456789In [18]: data1 = np.array([[1,2],[1,2]]) #生成一个2x2numpy数组 In [19]: data+data1---------------------------------------------------------------------------ValueError Traceback (most recent call last)&lt;ipython-input-19-f2592a975589&gt; in &lt;module&gt;()----&gt; 1 data+data1 ValueError: operands could not be broadcast together with shapes (3,3) (2,2) 我们可以看出不同规格的数组一起计算的话是会报出广播错误的，那是不是可以下结论了，别急我们再来看下方两个特殊例子： 123456789101112131415In [20]: data2 = np.array([[1,2,3]]) In [21]: data + data2Out[21]:array([[ 2, 4, 6], [ 5, 7, 9], [ 8, 10, 12]]) In [22]: data3 = np.array([[1],[2],[3]]) In [23]: data+data3Out[23]:array([[ 2, 3, 4], [ 6, 7, 8], [10, 11, 12]]) data2数组的列数量与data数组相等，data3数组的行数量与data数组相等，这两个numpy数组虽然规格与data数组不一样，但却依然可以与data数组进行运算。 数组的切片： 123456789101112In [24]: data[:2] #沿着行(axis=0)进行索引Out[24]:array([[1, 2, 3], [4, 5, 6]]) In [25]: data[:2,:2] #先沿着行(axis=0)进行索引，再沿着列(axis=1)进行索引Out[25]:array([[1, 2], [4, 5]]) In [26]: data[1,0:2] #下标是从0开始Out[26]: array([4, 5]) 这里需要注意的是，切片操作是在原始数组上创建一个视图view，这只是访问数组数据的一种方式。 因此原始数组不会被复制到内存中，传递的是一个类似引用的东西，与上面的astype（）方法是两种不同的拷贝方式，这里我们来看一个例子： 123456789101112In [32]: a = data[1] In [33]: aOut[33]: array([4, 5, 6]) In [34]: a[:] = 0 In [35]: dataOut[35]:array([[1, 2, 3], [0, 0, 0], [7, 8, 9]]) 当切片对象a改变时，data的对应值也会跟着改变，这是在我们日常数据处理中有时会疏忽的一个点，最安全的复制方法是使用 copy（）方法进行浅拷贝： 123456789101112In [36]: a = data[1].copy() In [37]: aOut[37]: array([0, 0, 0]) In [38]: a[:]=9 In [39]: dataOut[39]:array([[1, 2, 3], [0, 0, 0], [7, 8, 9]]) 数组的布尔索引： 1234567891011121314In [43]: dataOut[43]:array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) In [44]: data&gt;3Out[44]:array([[False, False, False], [ True, True, True], [ True, True, True]]) In [45]: data[data&gt;3] #找出大于3的元素Out[45]: array([4, 5, 6, 7, 8, 9]) 数组的逻辑表达处理： 12345In [46]: np.where(data&gt;3,1,0) #大于3的标记为1，小于等于3的标记为0Out[46]:array([[0, 0, 0], [1, 1, 1], [1, 1, 1]]) 数组的常用统计操作： 12345678910In [47]: data.mean(axis=0) #沿着行(axis=0)进行索引，求出其平均值 Out[47]: array([4., 5., 6.]) In [49]: data.std() #求出全部元素的方差 Out[49]: 2.581988897471611 In [50]: (data&gt;3).sum() #统计数组中元素大于3的个数 Out[50]: 6 In [51]: data.any() #数组中是否存在一个或多个true Out[51]: True In [52]: data.all() #数组中是否全部数都是true Out[52]: True 1234567891011In [53]: data.cumsum(0) #沿着行(axis=0)进行索引，进行累加Out[53]:array([[ 1, 2, 3], [ 5, 7, 9], [12, 15, 18]], dtype=int32) In [54]: data.cumprod(1) #沿着列(axis=1)进行索引，进行累乘Out[54]:array([[ 1, 2, 6], [ 4, 20, 120], [ 7, 56, 504]], dtype=int32) 数组的排序操作： 123456789101112131415161718192021222324In [55]: data=np.random.randn(4,4) In [56]: dataOut[56]:array([[ 1.58669867, 1.57692769, -1.85828013, 1.17201164], [ 1.68160714, -0.83957549, -0.33771694, -0.33782379], [-0.03148106, -0.97819034, 0.51126626, -0.08184963], [-0.02822319, -0.31934723, 0.70764701, 0.80444954]]) In [57]: data.sort(0) #沿着行(axis=0)进行索引，并进行升序排序 In [58]: dataOut[58]:array([[-0.03148106, -0.97819034, -1.85828013, -0.33782379], [-0.02822319, -0.83957549, -0.33771694, -0.08184963], [ 1.58669867, -0.31934723, 0.51126626, 0.80444954], [ 1.68160714, 1.57692769, 0.70764701, 1.17201164]]) In [59]: data[::-1] #降序操作Out[59]:array([[ 1.68160714, 1.57692769, 0.70764701, 1.17201164], [ 1.58669867, -0.31934723, 0.51126626, 0.80444954], [-0.02822319, -0.83957549, -0.33771694, -0.08184963], [-0.03148106, -0.97819034, -1.85828013, -0.33782379]]) 注意：直接调用数组的方法的排序将直接改变数组而不会产生新的拷贝。 矩阵运算： 12345678910111213141516171819In [62]: x=np.arange(9).reshape(3,3) In [63]: xOut[63]:array([[0, 1, 2], [3, 4, 5], [6, 7, 8]]) In [64]: np.dot(x,x) #矩阵相乘Out[64]:array([[ 15, 18, 21], [ 42, 54, 66], [ 69, 90, 111]]) In [65]: x.T #矩阵转置Out[65]:array([[0, 3, 6], [1, 4, 7], [2, 5, 8]]) 在numpy中的linalg中有还有很多矩阵运算，比如svd分解，qr分解，cholesky分解等等。 numpy数据的读取和保存： 12345678In [68]: np.save('name',data) In [69]: np.load('name.npy')Out[69]:array([[-0.03148106, -0.97819034, -1.85828013, -0.33782379], [-0.02822319, -0.83957549, -0.33771694, -0.08184963], [ 1.58669867, -0.31934723, 0.51126626, 0.80444954], [ 1.68160714, 1.57692769, 0.70764701, 1.17201164]]) 总结：numpy的知识学到这里，就已经可以应付好日常的数据处理了，便捷的操作，高效的处理，都使得numpy成为科学计算的一大利器。 参考资料：易百教程-numpy教程scipy讲义-numpyLEVEL-numpy ndarray详解 才学疏浅，欢迎评论指导","categories":[{"name":"Python","slug":"Python","permalink":"https://wengjj.ink/categories/Python/"}],"tags":[{"name":"numpy","slug":"numpy","permalink":"https://wengjj.ink/tags/numpy/"}]},{"title":"数据矿工学习-python基础思维导图1.0","slug":"数据矿工学习-python基础思维导图1.0","date":"2018-05-10T10:01:44.000Z","updated":"2020-08-30T08:01:41.805Z","comments":true,"path":"2018/05/10/数据矿工学习-python基础思维导图1.0/","link":"","permalink":"https://wengjj.ink/2018/05/10/%E6%95%B0%E6%8D%AE%E7%9F%BF%E5%B7%A5%E5%AD%A6%E4%B9%A0-python%E5%9F%BA%E7%A1%80%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE1.0/","excerpt":"python基础","text":"python基础 凹脑图在线浏览地址：python基础 才学疏浅，欢迎评论指导","categories":[{"name":"Python","slug":"Python","permalink":"https://wengjj.ink/categories/Python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://wengjj.ink/tags/python/"}]},{"title":"数据矿工学习-Python特性之迭代器与生成器","slug":"数据矿工学习-Python特性之迭代器与生成器","date":"2018-05-10T09:52:09.000Z","updated":"2020-08-30T08:02:39.858Z","comments":true,"path":"2018/05/10/数据矿工学习-Python特性之迭代器与生成器/","link":"","permalink":"https://wengjj.ink/2018/05/10/%E6%95%B0%E6%8D%AE%E7%9F%BF%E5%B7%A5%E5%AD%A6%E4%B9%A0-Python%E7%89%B9%E6%80%A7%E4%B9%8B%E8%BF%AD%E4%BB%A3%E5%99%A8%E4%B8%8E%E7%94%9F%E6%88%90%E5%99%A8/","excerpt":"迭代器要知道生成器是啥，首先得先了解下迭代器是什么，概念的部分还是用我最喜欢的老套路思维导图来表示：","text":"迭代器要知道生成器是啥，首先得先了解下迭代器是什么，概念的部分还是用我最喜欢的老套路思维导图来表示： 凹脑图在线浏览地址：迭代器 仔细看完这份思维导图后，我们需要区分好两个概念可迭代对象（iterable）与迭代器（iterator） 123num = [0,1,2,3,4] for i in num: print(i) 这里的列表num符合上面的条件之一：可以for循环，所以列表num可以称之为可迭代对象,那num可以说是迭代器吗？我们可以用isinstance方法来验证下： 123In [2]: from collections import Iterator In [3]: isinstance(num,Iterator) Out[3]: False 答案是False，因为列表num并不符合迭代器协议，简单点来讲，列表num里面并没有iter方法和next方法。下面我们按照迭代器协议要求自己来构造一个迭代器： 1234567891011121314151617class numIter: #迭代器 def __init__(self,n): self.n = n def __iter__(self): self.x = -1 return self def __next__(self): #Python 3.x版本 Python 2.x版本是next() self.x += 1 if self.x &lt; self.n: return self.x else: raise StopIteration for i in numIter(5): print(i) numIter里面包含了_iter_方法和_next_方法，符合了迭代器协议，numIter是不是迭代器呢？下面我们继续使用isinstance方法来验证： 12In [5]: isinstance(numIter,Iterator) Out[5]: False False！？！这里需要注意的是，numIter只是个类定义，本身是不会迭代的，而numIter(5)这个类的实例才可以进行迭代： 12In [7]: isinstance(numIter(5),Iterator) Out[7]: True 生成器生成器也是一种特殊的迭代器，概念部分继续惯例思维导图贴上： 凹脑图在线浏览地址：生成器 看完了思维导图，我们继续回到上面的那句话生成器也是一种特殊的迭代器，从上面的生成器运行流程中我们不难发现两个身影返回自身对象和next方法返回迭代值，这不就是我们上面迭代器讲的迭代器协议（iter方法和 next方法 ）吗？我们还是来用isinstance来验证一下 先用生成器表达式来生成一个表达器： 123In [13]: num = (i for i in range(5)) #注意这里使用的是（）不是[] In [14]: for i in num: ...: print(i) isinstance验证是否为迭代器： 12In [15]: isinstance(num,Iterator) Out[15]: True 答案为true，证明了生成器也是一种迭代器，那为什么要说生成器是一种特殊的迭代器呢？这时我们就得来看另一种生成器的生成方法-生成器函数： 12345def numGen(n): #生成器 x = 0 while x &lt; n: yield x x += 1 非常简短的几行代码，关键就在于yield这个关键字，一般来说如果我们的函数中出现了yield关键字，调用该函数时就会返回成一个生成器，为了更清楚地理解yield这个关键字的作用，我们还是用代码来说话： 12345678910111213In [19]: num = numGen(3) #得到一个生成器对象In [20]: print(num.__next__()) #执行next方法0 In [21]: print(num.__next__())1 In [22]: print(num.__next__())2 In [23]: print(num.__next__())---------------------------------------------------------------------------StopIteration Traceback (most recent call last) 首先我们运行第一行代码 1num = numGen(3) #得到一个生成器对象 得到一个生成器对象，很容易理解的一行代码，但当我们与普通的return方法进行对比时，我们就会发现一个有趣的现象： 12345678910111213141516def numGen(n): #生成器 x = 0 print(\"生成器执行中\") while x &lt; n: yield x x += 1 def numGen1(n): x = 0 print(\"普通方法执行中\") while x &lt; n: x += 1 return xnum = numGen(3)num1 = numGen1(3)输出：普通方法执行中 从这个例子我们就可以看出，当我们生成一个生成器对象时，生成器函数内部的代码并不会马上执行，而普通return函数生成对象时即开始运行内部代码，那生成器函数的代码时什么时候开始执行的呢？别急我们来运行下一行代码： 123In [25]: print(num.__next__() 生成器执行中0 得出答案，生成器函数的内部代码是在执行next（）方法后才开始执行的，新的问题又出现了代码是执行到关键字yield就暂停还是整段代码运行完才暂停，这里我们将上面的例子再次改装（然而我第一次在ipython运行时遇到了一个“bug”，代码如下）： 1234567891011In [26]: def numGen(n): #生成器 ...: x = 0 ...: print(\"生成器执行yield前\") ...: while x &lt; n: ...: yield x ...: print(\"生成器执行yield后\") ...: x += 1 ...: In [27]: print(num.__next__())1 这其实不是bug，这是生成器的·一个特性：只可以读取一次，所以这里得再重新运行一次： 123456789101112131415161718192021222324252627In [1]: def numGen(n): #生成器 ...: x = 0 ...: print(\"生成器执行yield前\") ...: while x &lt; n: ...: yield x ...: print(\"生成器执行yield后\") ...: x += 1 ...: In [2]: num = numGen(3) In [3]: print(num.__next__())生成器执行yield前0 In [4]: print(num.__next__())生成器执行yield后1 In [5]: print(num.__next__())生成器执行yield后2 In [6]: print(num.__next__())生成器执行yield后---------------------------------------------------------------------------StopIteration Traceback (most recent call last) 有了这个例子，我们就能很好地理解关键字yield的作用了，当代码运行到关键字yield时，执行中断并返回当前的迭代值，除此之外当前的上下文环境也会被记录下来，简单点讲就是执行中断的位置和数据都被保存起来。再次使用** next() 的时候，从原来中断的地方继续执行，直至遇到 **yield，如果没有** yield**，则抛出StopIteration 异常。 了解了生成器的运行机制，最后我们再来了解下生成器其余的三种方法: send（）方法1234567891011121314151617181920In [3]: def numGen(n): #生成器 ...: x = 0 ...: while x &lt; n: ...: y = yield x ...: print(y) ...: x += 1 ...: num = numGen(3) ...: print(num.__next__()) ...: print(num.send(999)) ...: print(num.__next__()) ...: print(num.__next__()) ...:09991None2None---------------------------------------------------------------------------StopIteration Traceback (most recent call last) 下面来说下运行流程： 首先调用next（）方法，让生成器内部代码执行到关键字yield处，返回0； 接着调用send（999）方法，将值999传到代码执行中断的地方，也就是关键字yield处，将999赋值给y，输出y，执行x+=1，执行到关键字yield处，返回1； 继续调用next（）方法，无值赋给y，y=None，输出y，执行x+=1，执行到关键字yield处，返回2； 继续调用next（）方法，无值赋给y，y=None，输出y，执行x+=1,x=n跳出while循环，找不到关键字yield，抛出StopIteration 异常； throw（）方法123456789101112131415161718192021In [4]: def numGen(n): #生成器 ...: try: ...: x = 0 ...: while x &lt; n: ...: yield x ...: x += 1 ...: except ValueError: ...: yield 'Error' ...: finally: ...: print('Finally') ...: ...: num = numGen(3) ...: print(num.__next__()) ...: print(num.throw(ValueError)) ...: print(num.__next__()) ...:0ErrorFinally---------------------------------------------------------------------------StopIteration Traceback (most recent call last) 可以看出当我们向生成器抛去ValueError错误时，整个生成器就执行finally，最后抛出StopIteration 异常； close（）方法1234567891011121314In [5]: def numGen(n): #生成器 ...: x = 0 ...: while x &lt; n: ...: yield x ...: x += 1 ...: ...: num = numGen(3) ...: print(num.__next__()) ...: num.close() ...: print(num.__next__()) ...:0---------------------------------------------------------------------------StopIteration Traceback (most recent call last) 当我们运行close（）方法时，整个生成器就终止了，再执行next（）方法，就抛出StopIteration 异常； 最后，学了这么多，生成器到底有什么过人之处：1）由于生成器这种“走停走停”策略，使得生成器可以逐步生成序列，不用像list一样初始化时就要开辟所有的空间，所以当你一次只需对一个数进行处理时，使用生成器是一个不错的选择。 2）运用好生成器的四种方法next（）throw（）send（）close（）还有生成器的关键字yield的特性，是可以实现伪并发操作的，Python虽然支持多线程，可由于GIL（全局解释锁）的存在，使得同一时刻只能有一条线程运行，并没有办法并行操作，所以Python的多线程实际上就是鸡肋。 3）我们在读取文件时，如果直接对文件对象调用 read() 方法，会导致不可预测的内存占用。好的方法是利用固定长度的缓冲区来不断读取文件内容。通过 yield，我们不再需要编写读文件的迭代类，就可以轻松实现文件读取： 下面贴上廖雪峰老师的yield读取文件代码： 123456789def read_file(fpath): BLOCK_SIZE = 1024 with open(fpath, 'rb') as f: while True: block = f.read(BLOCK_SIZE) if block: yield block else: return 参考资料：廖雪峰老师的Python yield 使用浅析Billy.J.Hee的技术博客(译)Python关键字yield的解释(stackoverflow) 才学疏浅，欢迎评论指导","categories":[{"name":"Python","slug":"Python","permalink":"https://wengjj.ink/categories/Python/"}],"tags":[{"name":"迭代器","slug":"迭代器","permalink":"https://wengjj.ink/tags/%E8%BF%AD%E4%BB%A3%E5%99%A8/"},{"name":"生成器","slug":"生成器","permalink":"https://wengjj.ink/tags/%E7%94%9F%E6%88%90%E5%99%A8/"}]},{"title":"数据矿工学习-《统计学习方法》思维导图5.0-决策树","slug":"数据矿工学习-《统计学习方法》思维导图5.0-决策树","date":"2018-05-05T18:20:00.000Z","updated":"2020-08-30T06:53:20.509Z","comments":true,"path":"2018/05/06/数据矿工学习-《统计学习方法》思维导图5.0-决策树/","link":"","permalink":"https://wengjj.ink/2018/05/06/%E6%95%B0%E6%8D%AE%E7%9F%BF%E5%B7%A5%E5%AD%A6%E4%B9%A0-%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE5.0-%E5%86%B3%E7%AD%96%E6%A0%91/","excerpt":"本章主要讲了决策树学习算法的三个部分：特征选择、树的生成方式和树的剪枝以及三种决策树算法：ID3、C4.5、CART Tips:决策树的东西比较多（这章啃了一个星期才啃完 (lll￢ω￢)），我们平时使用的Sklearn里面的决策树用的就是CART算法，所以这章的重点就在于CART的两种树模型（回归树、分类树），ID3和C4.5可作为了解。","text":"本章主要讲了决策树学习算法的三个部分：特征选择、树的生成方式和树的剪枝以及三种决策树算法：ID3、C4.5、CART Tips:决策树的东西比较多（这章啃了一个星期才啃完 (lll￢ω￢)），我们平时使用的Sklearn里面的决策树用的就是CART算法，所以这章的重点就在于CART的两种树模型（回归树、分类树），ID3和C4.5可作为了解。 思维来自《统计学习方法》-李航 凹脑图在线浏览地址：决策树 **才学疏浅，欢迎评论指导**","categories":[{"name":"《统计学习方法-李航》","slug":"《统计学习方法-李航》","permalink":"https://wengjj.ink/categories/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9D%8E%E8%88%AA%E3%80%8B/"}],"tags":[{"name":"决策树","slug":"决策树","permalink":"https://wengjj.ink/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"}]},{"title":"数据矿工学习-《统计学习方法》思维导图4.0-朴素贝叶斯法","slug":"数据矿工学习-《统计学习方法》思维导图4.0-朴素贝叶斯法","date":"2018-04-20T05:08:16.000Z","updated":"2020-08-30T06:53:36.937Z","comments":true,"path":"2018/04/20/数据矿工学习-《统计学习方法》思维导图4.0-朴素贝叶斯法/","link":"","permalink":"https://wengjj.ink/2018/04/20/%E6%95%B0%E6%8D%AE%E7%9F%BF%E5%B7%A5%E5%AD%A6%E4%B9%A0-%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE4.0-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/","excerpt":"本章主要讲了生成学习方法-朴素贝叶斯法的详细生成方法 、 朴素贝叶斯的特点-条件独立性假设","text":"本章主要讲了生成学习方法-朴素贝叶斯法的详细生成方法 、 朴素贝叶斯的特点-条件独立性假设 思维来自《统计学习方法》-李航 凹脑图在线浏览地址：朴素贝叶斯思维导图 **才学疏浅，欢迎评论指导**","categories":[{"name":"《统计学习方法-李航》","slug":"《统计学习方法-李航》","permalink":"https://wengjj.ink/categories/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9D%8E%E8%88%AA%E3%80%8B/"}],"tags":[{"name":"朴素贝叶斯法","slug":"朴素贝叶斯法","permalink":"https://wengjj.ink/tags/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/"}]},{"title":"数据矿工学习-《统计学习方法》思维导图3.0-K近邻法","slug":"数据矿工学习-《统计学习方法》思维导图3.0-K近邻法","date":"2018-04-11T14:48:01.000Z","updated":"2020-08-30T07:50:13.060Z","comments":true,"path":"2018/04/11/数据矿工学习-《统计学习方法》思维导图3.0-K近邻法/","link":"","permalink":"https://wengjj.ink/2018/04/11/%E6%95%B0%E6%8D%AE%E7%9F%BF%E5%B7%A5%E5%AD%A6%E4%B9%A0-%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE3.0-K%E8%BF%91%E9%82%BB%E6%B3%95/","excerpt":"本章主要讲了K近邻的算法、模型 以及kd树的构造与最近邻搜索。","text":"本章主要讲了K近邻的算法、模型 以及kd树的构造与最近邻搜索。 思维来自《统计学习方法》-李航 凹脑图在线浏览地址：K近邻法思维导图才学疏浅，欢迎评论指导","categories":[{"name":"《统计学习方法-李航》","slug":"《统计学习方法-李航》","permalink":"https://wengjj.ink/categories/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9D%8E%E8%88%AA%E3%80%8B/"}],"tags":[{"name":"K近邻法","slug":"K近邻法","permalink":"https://wengjj.ink/tags/K%E8%BF%91%E9%82%BB%E6%B3%95/"}]},{"title":"数据矿工学习-《统计学习方法》思维导图2.0-感知机","slug":"数据矿工学习-《统计学习方法》思维导图2.0-感知机","date":"2018-03-31T18:19:51.000Z","updated":"2020-08-30T07:50:33.958Z","comments":true,"path":"2018/04/01/数据矿工学习-《统计学习方法》思维导图2.0-感知机/","link":"","permalink":"https://wengjj.ink/2018/04/01/%E6%95%B0%E6%8D%AE%E7%9F%BF%E5%B7%A5%E5%AD%A6%E4%B9%A0-%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE2.0-%E6%84%9F%E7%9F%A5%E6%9C%BA/","excerpt":"本章主要讲了感知机的模型、策略以及算法的相关证明。","text":"本章主要讲了感知机的模型、策略以及算法的相关证明。 思维来自《统计学习方法》-李航 凹脑图在线浏览地址：感知机思维导图才学疏浅，欢迎评论指导","categories":[{"name":"《统计学习方法-李航》","slug":"《统计学习方法-李航》","permalink":"https://wengjj.ink/categories/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9D%8E%E8%88%AA%E3%80%8B/"}],"tags":[{"name":"感知机","slug":"感知机","permalink":"https://wengjj.ink/tags/%E6%84%9F%E7%9F%A5%E6%9C%BA/"}]},{"title":"数据矿工学习-数据挖掘思维导图1.0","slug":"数据矿工学习-数据挖掘思维导图1.0","date":"2018-03-28T12:18:16.000Z","updated":"2020-08-30T07:50:57.432Z","comments":true,"path":"2018/03/28/数据矿工学习-数据挖掘思维导图1.0/","link":"","permalink":"https://wengjj.ink/2018/03/28/%E6%95%B0%E6%8D%AE%E7%9F%BF%E5%B7%A5%E5%AD%A6%E4%B9%A0-%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE1.0/","excerpt":"本思维导图主要介绍 数据挖掘 的几个基本流程","text":"本思维导图主要介绍 数据挖掘 的几个基本流程 才学疏浅，欢迎评论指导","categories":[{"name":"数据挖掘","slug":"数据挖掘","permalink":"https://wengjj.ink/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"}],"tags":[{"name":"数据挖掘","slug":"数据挖掘","permalink":"https://wengjj.ink/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"}]},{"title":"数据矿工学习-《统计学习方法》思维导图1.2-模型的评估与选择","slug":"数据矿工学习-《统计学习方法》思维导图1.2-模型的评估与选择","date":"2018-03-28T03:09:33.000Z","updated":"2020-08-30T07:51:17.191Z","comments":true,"path":"2018/03/28/数据矿工学习-《统计学习方法》思维导图1.2-模型的评估与选择/","link":"","permalink":"https://wengjj.ink/2018/03/28/%E6%95%B0%E6%8D%AE%E7%9F%BF%E5%B7%A5%E5%AD%A6%E4%B9%A0-%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE1.2-%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/","excerpt":"《统计学习概论》第一章到这里就结束了，本思维导图主要讲的是模型的选择，模型的评估还有统计学习三大问题。","text":"《统计学习概论》第一章到这里就结束了，本思维导图主要讲的是模型的选择，模型的评估还有统计学习三大问题。 思维来自《统计学习方法》-李航 凹脑图在线浏览地址：模型评估与选择才学疏浅，欢迎评论指导","categories":[{"name":"《统计学习方法-李航》","slug":"《统计学习方法-李航》","permalink":"https://wengjj.ink/categories/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9D%8E%E8%88%AA%E3%80%8B/"}],"tags":[{"name":"模型的评估与选择","slug":"模型的评估与选择","permalink":"https://wengjj.ink/tags/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/"}]},{"title":"数据矿工学习-《统计学习方法》思维导图1.1-监督学习与统计学习三要素","slug":"数据矿工学习-《统计学习方法》思维导图1.1-监督学习与统计学习三要素","date":"2018-03-28T03:08:54.000Z","updated":"2020-08-30T07:51:32.265Z","comments":true,"path":"2018/03/28/数据矿工学习-《统计学习方法》思维导图1.1-监督学习与统计学习三要素/","link":"","permalink":"https://wengjj.ink/2018/03/28/%E6%95%B0%E6%8D%AE%E7%9F%BF%E5%B7%A5%E5%AD%A6%E4%B9%A0-%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE1.1-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E4%B8%89%E8%A6%81%E7%B4%A0/","excerpt":"监督学习概论 与 统计学习三要素","text":"监督学习概论 与 统计学习三要素 思维来自《统计学习方法》-李航 凹脑图在线浏览地址：统计学习三要素才学疏浅，欢迎评论指导","categories":[{"name":"《统计学习方法-李航》","slug":"《统计学习方法-李航》","permalink":"https://wengjj.ink/categories/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9D%8E%E8%88%AA%E3%80%8B/"}],"tags":[{"name":"监督学习与统计学习三要素","slug":"监督学习与统计学习三要素","permalink":"https://wengjj.ink/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E4%B8%89%E8%A6%81%E7%B4%A0/"}]},{"title":"数据矿工学习-《统计学习方法》思维导图1.0-统计学习概论","slug":"数据矿工学习-《统计学习方法》思维导图1.0-统计学习概论","date":"2018-03-28T03:07:39.000Z","updated":"2020-08-30T07:51:48.541Z","comments":true,"path":"2018/03/28/数据矿工学习-《统计学习方法》思维导图1.0-统计学习概论/","link":"","permalink":"https://wengjj.ink/2018/03/28/%E6%95%B0%E6%8D%AE%E7%9F%BF%E5%B7%A5%E5%AD%A6%E4%B9%A0-%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E3%80%8B%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE1.0-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/","excerpt":"","text":"改进了思维导图了，尽量简约为主，不过多延伸。 思维来自《统计学习方法》-李航 凹脑图在线浏览地址：统计学习方法概率才学疏浅，欢迎评论指导","categories":[{"name":"《统计学习方法-李航》","slug":"《统计学习方法-李航》","permalink":"https://wengjj.ink/categories/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9D%8E%E8%88%AA%E3%80%8B/"}],"tags":[{"name":"统计学习概论","slug":"统计学习概论","permalink":"https://wengjj.ink/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/"}]},{"title":"Hello World","slug":"hello-world","date":"2017-08-30T05:28:47.000Z","updated":"2020-08-30T07:52:09.388Z","comments":true,"path":"2017/08/30/hello-world/","link":"","permalink":"https://wengjj.ink/2017/08/30/hello-world/","excerpt":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"C++","slug":"C","permalink":"https://wengjj.ink/categories/C/"},{"name":"数据挖掘","slug":"数据挖掘","permalink":"https://wengjj.ink/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"},{"name":"《统计学习方法-李航》","slug":"《统计学习方法-李航》","permalink":"https://wengjj.ink/categories/%E3%80%8A%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9D%8E%E8%88%AA%E3%80%8B/"},{"name":"Python","slug":"Python","permalink":"https://wengjj.ink/categories/Python/"},{"name":"深度学习","slug":"深度学习","permalink":"https://wengjj.ink/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"论文翻译","slug":"深度学习/论文翻译","permalink":"https://wengjj.ink/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/"},{"name":"比赛项目","slug":"比赛项目","permalink":"https://wengjj.ink/categories/%E6%AF%94%E8%B5%9B%E9%A1%B9%E7%9B%AE/"},{"name":"图像识别","slug":"图像识别","permalink":"https://wengjj.ink/categories/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/"},{"name":"论文翻译","slug":"图像识别/论文翻译","permalink":"https://wengjj.ink/categories/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/"},{"name":"论文简析","slug":"图像识别/论文简析","permalink":"https://wengjj.ink/categories/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/%E8%AE%BA%E6%96%87%E7%AE%80%E6%9E%90/"},{"name":"NLP","slug":"NLP","permalink":"https://wengjj.ink/categories/NLP/"},{"name":"论文翻译","slug":"NLP/论文翻译","permalink":"https://wengjj.ink/categories/NLP/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/"},{"name":"论文简析","slug":"NLP/论文简析","permalink":"https://wengjj.ink/categories/NLP/%E8%AE%BA%E6%96%87%E7%AE%80%E6%9E%90/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://wengjj.ink/tags/C/"},{"name":"数据拆分","slug":"数据拆分","permalink":"https://wengjj.ink/tags/%E6%95%B0%E6%8D%AE%E6%8B%86%E5%88%86/"},{"name":"特征缩放","slug":"特征缩放","permalink":"https://wengjj.ink/tags/%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE/"},{"name":"SMO序列最小最优化算法","slug":"SMO序列最小最优化算法","permalink":"https://wengjj.ink/tags/SMO%E5%BA%8F%E5%88%97%E6%9C%80%E5%B0%8F%E6%9C%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"},{"name":"pyecharts","slug":"pyecharts","permalink":"https://wengjj.ink/tags/pyecharts/"},{"name":"深度学习","slug":"深度学习","permalink":"https://wengjj.ink/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"100天机器学习挑战","slug":"100天机器学习挑战","permalink":"https://wengjj.ink/tags/100%E5%A4%A9%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8C%91%E6%88%98/"},{"name":"非线性支持向量机","slug":"非线性支持向量机","permalink":"https://wengjj.ink/tags/%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"},{"name":"核函数","slug":"核函数","permalink":"https://wengjj.ink/tags/%E6%A0%B8%E5%87%BD%E6%95%B0/"},{"name":"线性支持向量机","slug":"线性支持向量机","permalink":"https://wengjj.ink/tags/%E7%BA%BF%E6%80%A7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"},{"name":"样本自适应的在线卷积稀疏编码","slug":"样本自适应的在线卷积稀疏编码","permalink":"https://wengjj.ink/tags/%E6%A0%B7%E6%9C%AC%E8%87%AA%E9%80%82%E5%BA%94%E7%9A%84%E5%9C%A8%E7%BA%BF%E5%8D%B7%E7%A7%AF%E7%A8%80%E7%96%8F%E7%BC%96%E7%A0%81/"},{"name":"线性可分支持向量机","slug":"线性可分支持向量机","permalink":"https://wengjj.ink/tags/%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"},{"name":"情感分析框架DeepEmo","slug":"情感分析框架DeepEmo","permalink":"https://wengjj.ink/tags/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E6%A1%86%E6%9E%B6DeepEmo/"},{"name":"最大熵模型","slug":"最大熵模型","permalink":"https://wengjj.ink/tags/%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/"},{"name":"最优化算法","slug":"最优化算法","permalink":"https://wengjj.ink/tags/%E6%9C%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"},{"name":"逻辑斯蒂回归模型","slug":"逻辑斯蒂回归模型","permalink":"https://wengjj.ink/tags/%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/"},{"name":"pandas","slug":"pandas","permalink":"https://wengjj.ink/tags/pandas/"},{"name":"numpy","slug":"numpy","permalink":"https://wengjj.ink/tags/numpy/"},{"name":"python","slug":"python","permalink":"https://wengjj.ink/tags/python/"},{"name":"迭代器","slug":"迭代器","permalink":"https://wengjj.ink/tags/%E8%BF%AD%E4%BB%A3%E5%99%A8/"},{"name":"生成器","slug":"生成器","permalink":"https://wengjj.ink/tags/%E7%94%9F%E6%88%90%E5%99%A8/"},{"name":"决策树","slug":"决策树","permalink":"https://wengjj.ink/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"},{"name":"朴素贝叶斯法","slug":"朴素贝叶斯法","permalink":"https://wengjj.ink/tags/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/"},{"name":"K近邻法","slug":"K近邻法","permalink":"https://wengjj.ink/tags/K%E8%BF%91%E9%82%BB%E6%B3%95/"},{"name":"感知机","slug":"感知机","permalink":"https://wengjj.ink/tags/%E6%84%9F%E7%9F%A5%E6%9C%BA/"},{"name":"数据挖掘","slug":"数据挖掘","permalink":"https://wengjj.ink/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"},{"name":"模型的评估与选择","slug":"模型的评估与选择","permalink":"https://wengjj.ink/tags/%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%80%89%E6%8B%A9/"},{"name":"监督学习与统计学习三要素","slug":"监督学习与统计学习三要素","permalink":"https://wengjj.ink/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E4%B8%89%E8%A6%81%E7%B4%A0/"},{"name":"统计学习概论","slug":"统计学习概论","permalink":"https://wengjj.ink/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/"}]}